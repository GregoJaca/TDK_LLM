\documentclass[a4paper,12pt]{article}
\usepackage{graphicx,amssymb,textcase,fancyhdr,enumerate,wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Use T1 encoding
\usepackage{mathptmx} % Use Times New Roman font
\usepackage[margin=2cm]{geometry} % Set margins to 2 cm
\usepackage{setspace} % For line spacing
\setstretch{1.4} % Set line spacing to 1.4
\usepackage{icomma}
\usepackage{xcolor}
\usepackage{bm}
\usepackage[unicode,colorlinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=green,      
    urlcolor=blue,
}

\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{mathtools}
\usepackage[normalem]{ulem}

\usepackage{subcaption}
\usepackage{amsmath} % for the cases environment
\usepackage{amsfonts} % additional math symbols (if needed)
\usepackage{graphicx}

\DeclarePairedDelimiter\abs{|}{|}
\DeclareMathOperator{\tg}{tg}
\definecolor{new_red}{HTML}{ff0000}
\definecolor{new_blue}{HTML}{0000ff}
\definecolor{new_green}{HTML}{009a00}
\definecolor{new_orange}{HTML}{ff8000}

\colorlet{light_red}{new_red!20!white}
\colorlet{light_blue}{new_blue!20!white}
\colorlet{light_green}{new_green!20!white}
\colorlet{light_orange}{new_orange!20!white}
\newcommand{\JT}[1]{{\color{blue} #1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vv}{\vect{v}}
\newcommand{\vu}{\vect{u}}

\newcommand{\hlb}[1]{\mathpalette\highlightb{#1}}
\newcommand{\highlightb}[2]{\colorbox{light_blue}{$#1#2$}}

\newcommand{\hlr}[1]{\mathpalette\highlightr{#1}}
\newcommand{\highlightr}[2]{\colorbox{light_red}{$#1#2$}}

\newcommand{\hlo}[1]{\mathpalette\highlighto{#1}}
\newcommand{\highlighto}[2]{\colorbox{light_orange}{$#1#2$}}

\newcommand{\hlg}[1]{\mathpalette\highlightg{#1}}
\newcommand{\highlightg}[2]{\colorbox{light_green}{$#1#2$}}

\setlength{\parskip}{0pt plus 0pt}
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.1}

\sloppy


\begin{document}

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\huge\textsc{TDK}
\HRule \\[0.4cm]
{ \huge \bfseries Chaos in Large Language Models}\\[0.2cm] 
\HRule \\[0.5cm]
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
Author: \\
\Large\textbf{Jaca Gregorio}\\
\large{Physicist Engineering BSc, III. year.}
\end{flushleft}
\end{minipage}
\qquad
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft}\large
Supervisors: \\
\Large\textbf{Dr. Török János}\\
\large Associate professor \\
BME Elméleti Fizika Tanszék

\Large\textbf{Kristóf Benedek}\\
\large ? \\
?
\end{flushleft}
\end{minipage}
\\[6cm]
\includegraphics[scale=0.7]{plots/bme.png}\\[0.2cm]
\large{\textbf{BME}}\\
\large{\textbf{2025}}
\vfill
\end{titlepage}

% \newpage\null\thispagestyle{empty}\newpage

%%% --------- GUIDELINES ---------- %%%
% This doc is in progress. I write but leave many comments to remind myself where to go back and improve. Wherever you see a "(GGG)" it means that I'm not happy with the wording and it needs to be made more precise and clear.
% When I use "G:" i am straightforwardly expressing an opinion 
% It's easier for me if you use your initials before your comments, so i can quickly understand who they come from

% Things I'm working on:
% Intro is very bad
% Results needs more quantitative data: RQA table and maybe some lyapunov fit if possible
% I have too many sections and things. I will at the end remove the extras. If you think anything in the main text should not be there, leave a comment.
% I'm still developing this idea that "stretching and folding" which is intuitively what happens in chaotic systems, and which parts of the LLM architecture cause it
% explain better why and how sliding window are used for the distance metrics. how this is analogous to the sentence embeddings (where a whole sentence is converted into a vector). and how this helps account for the context which makes sense to do because LLMs are more like a delay system, where the state is not defined by only a single hidden state, but also the ones in its context window. I will just mention this, which I think is reasonable.
% there are some metrics missing in methods. I'm still not 100% which I'll include and which not. the ones currently in methods I will probably include (except "PCD", which maybe won't)
% I will adress all 'GGG' marked issues

% Sections which are mostly finished:
% Related Work - Preliminaries - Methods

% GENERAL TODO:
% add labels to each section, subsection, plot
% introduce abbreviations only once. more consistency in notation


\begin{abstract}
    Large Language Models (LLMs) have achieved remarkable performance across a wide range of tasks, yet their internal dynamics remain poorly understood. In this work, we apply the tools of nonlinear dynamics and chaos theory to LLMs. By analyzing both text and hidden state trajectories, we demonstrate that LLMs exhibit hallmark signatures of chaos, including sensitivity to initial conditions and a positive maximum Lyapunov exponent, with consistent results across different distance metrics. Recurrence plots show structural similarities between LLMs and canonical chaotic systems such as the Lorenz attractor, while dimension analysis reveals fractal structures in the hidden state space, particularly pronounced in the last layers. We propose that the nonlinear coupling induced by attention mechanisms plays a key role in driving this chaotic behavior. 
\end{abstract}
\newpage
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\newpage

\section{Introduction and Motivation}
\label{sec:introduction}

% G: This paragraph wants to say: Mathematicians study silly nonlinear diff eqs for fun. LLMs are even more interesting.
% we could frame the LLM as a non-autonomous dynamical system, where the input sequence (prompt) acts as a time-dependent driving force. This distinguishes it from classical autonomous systems like the Lorenz attractor and might have implications for the stability and types of attractors we observe. ??
Deep Neural Networks (DNNs), such as LLMs, are highly nonlinear systems whose parameters are not explicitly determined, but rather they are learned by training on huge sets of data.
This allows these models to learn huge amounts of knowledge with little human intervention, which allows for scaling at the cost of interpretability.
DNNs provide a fascinating case study for nonlinear dynamics.
% Studying these systems is extremely interesting
% A common theme with LLMs is that there is very little that we explicitly instruct them to do, rather we create the architecture and they learn everything during trainig

There is growing interest in the sensitivity of LLMs (and other generative deep neural networks) to initial conditions and small perturbations. For some applications, like Reinforcement Learning (RL), determinism and reproducibility are desirable in order to keep the RL on-policy, while for other uses like synthetic data generation, paraphrasing, and novel image generation, diversity is desired.
% Understanding fixed/stable points in LLMs can clarify their generative capability constraints.
% G: needs a more natural phrasing, not forcing it into the SIC chaos direction until the end
% Does the model operate in a regime that is chaotic enough to allow for creativity, but stable enough to maintain coherence? 

% G: I want to say: LLMs are trained on all human data. what they learn is representative of human civilization (if we consider the written data is). more insights into LLMs is insights into us. maybe read motivation for some interpretability articles
% G: but this is not so much within the scope of this study
Given that LLMs are trained of a large corpus of human data, what they know says a lot about humanity, at least what is captured in text. Improving our understanding of the inner representations of LLMs yields insights about us.

% G: also these tools can be applied to other DNNs that handle audio image video
% this dynamical systems perspective is a general framework applicable to any generative model that operates sequentially, not just text-based LLMs.

Much research interest has been placed on interpretability, understanding how LLMs learn and store facts
LLMs are not created, they are grown and trained
Interpretability focuses on completely understanding small components (called circuits) and their patters and behaviors in neural networks, and we believe the tools from chaos theory can help shed light into these processes \cite{olah2020zoom} \cite{ameisen2025circuit} \cite{lindsey2025biology}. The tools from chaos theory can be applied to small and large models alike. % Emergent properties at large scales could be identified using chaos metrics.

Working with LLMs presents some challenges, such as the discrete nature of tokens and their hidden state representations, and the high dimensionality of the embedding space. 
We worked around many of these challenges and we hope this experience can be transferred to the study of other high dimensional chaotic systems.
% G: add more challenges
% Another challenge is the non-stationarity. The 'rules' of the system (the attention patterns) are state-dependent, changing at every single step. This is unlike classic chaotic systems where the governing equations are fixed. This makes analysis much harder but also much more interesting.

\section{Related Work} 
\label{sec:related_work}
% This section quite common in ML papers. 
% This section briefly summarizes the main results from other papers which inspired my analysis and also explain some of the results obtained. It also should reinforce the idea that it's justifiable to treat LLMs and chaotic nonlinear systems. 

The application of dynamical systems theory to neural networks has revealed complex behaviors in these models. 
% We see this work as an expansion of this, focusing on LLMs and bringing the tools from chaos theory.

\paragraph{Chaotic Dynamics in LLMs:}

\cite{li2025cognitive_activation} proposes that LLMs' reasoning capabilities as chaotic processes of dynamic information extraction in parameter space, introducing Quasi-Lyapunov Exponents to quantify chaotic characteristics across model layers, and showing sensitivity to initial conditions.
\cite{geshkovski2025mathematicalperspectivetransformers} model self-attention as a nonlinear coupling between tokens.
\cite{dynamicalmeanfieldtheoryselfattention} demonstrate that even simplified self-attention transformer networks with 1-bit tokens and weights exhibit nontrivial dynamical phenomena, including non-equilibrium phase transitions and chaotic bifurcations. 
\cite{tomihari2025recurrent_self_attention_dynamics} show that normalization layers normalize Jacobian's complex eigenvalues bringing the dynamics close to a critical state with maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos: a critical regime where signals neither explode nor vanish, enabling long-range information propagation. % GGG check into which paragraph to put
% G: this can explain if the trajectories have sub-exponential divergence
%show that normalization layers effectively suppress the Jacobian's spectral norm and control oscillatory behaviors. Their empirical findings reveal that high-performance self-attention models exhibit maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos—a critical regime where signals neither explode nor vanish, enabling long-range information propagation.
% this provides a mechanistic explanation for why the system might hover at the edge of chaos. The normalization layers act as a dissipative force, counteracting the explosive stretching from other parts of the network, keeping the system bounded and stable. This connects directly to the idea of 'folding'.

\paragraph{Determinism and Attractors in Language Models:}
% The tension between reproducibility and novely in LLMs has become increasingly relevant. 
\cite{he2025nondeterminism}
Recent engineering efforts have focused on achieving deterministic inference by controlling GPU batching variance and floating-point errors, motivated by the need for reproducibility and clear signals in reinforcement learning training. Understanding LLM's sensitivity to perturbations is useful for this branch of study.
 %Knowing the LLMs sensitivity to disturbances is important for this.
 %This highlights the practical sensitivity of LLMs to small numerical perturbations. 
\cite{wang2025unveiling_attractor_cycles} show that LLMs used for paraphrasing converge to periodic attractor cycles, reducing linguistic diversity.
\cite{cyclegan} % Comment: PLOS Complex Systems article - 
demonstrate that CycleGAN image generators' space is more limited than the training data's, having positive Lyapunov exponents and attractor dimensions similar to the training data intrinsic dimension. Chaotic dynamics contribute to the diversity of the generated images.
%This suggests that generative models may naturally converge to stable configurations that reduce output diversity. 
% G: this is a bit contradictory bc I say that model generation: more limited than training data but chaotic makes more diverse.
% This isn't necessarily a contradiction. The system can be chaotic (positive Lyapunov exponent, sensitive dependence) which allows for diversity, but still be confined to an attractor whose dimension is smaller than the ambient space. This is the definition of a *strange attractor*. The chaos creates novelty *within* the bounds of the learned manifold.

\paragraph{Interpretability Through Dynamical Analysis:}
\label{par:interpretability_dynamics}
%The dynamical systems perspective has proven valuable for understanding recurrent architectures. 
\cite{sussillo2013} showed that fixed points and linearized dynamics in trained RNNs provide interpretable insights into network function. \cite{zhang2024intelligence_edge_of_chaos} found that intelligence emerges at an optimal complexity level. Systems that are either highly chaotic or perfectly periodic exhibit poor downstream performance, suggesting a "sweet spot" conducive to intelligent behavior at the edge of chaos. \cite{zhou2025geometryreasoningflowinglogics} show how LLM's chain-of-thought reasoning can be interpreted as smooth flows in embedding space, where logical statements control the flow's velocities.
% G: grego this section and see where in the text you should cite this papers etc...

\section{Preliminaries}
\label{sec:preliminaries}

% This section describes the background knowledge required to understand Nonlinear Dynamics and Chaos, LLMs, and their similarities and differences. This provides us with the starting point for this paper.

\subsection{Nonlinear Dynamics and Chaos}
\label{subsec:nonlinear_dynamics}

% GGG todo improve this start
A dynamical system describes the evolution of a point over time in a geometrical space called phase space. The system is called deterministic if its future evolution depends only on its current state. The path traced by the point is called a trajectory. % end
Such systems can be described by differential equations in the case of continuous time, or by iterated maps for discrete time steps. Nonlinearity is a crucial property, as it is a necessary condition for complex behaviors like chaos to emerge. % GGG improve paragraph

Chaotic systems are a subclass of deterministic nonlinear systems that exhibit complex and practically unpredictable behavior. This apparent randomness arises not from a stochastic nature, but from the system's intrinsic dynamics. The defining characteristics of chaos are non-periodicity and sensitivity to initial conditions (SIC). This property implies that trajectories originating from infinitesimally close initial states, say $\mathbf{x}(0)$ and $\mathbf{x}(0) + \bm{\delta}(0)$, will diverge exponentially over time. The separation distance $\|\bm{\delta}(t)\|$ grows according to:
\begin{equation}
    \|\bm{\delta}(t)\| \approx \|\bm{\delta}(0)\| e^{\lambda t}
\end{equation}
where $\lambda$ is the maximal Lyapunov exponent. A positive maximal Lyapunov exponent is a strong indicator of chaos. It signifies that any measurement error or uncertainty in the initial state, no matter how small, will be amplified exponentially, making long-term prediction impossible.

Despite their practical unpredictability, the trajectories of dissipative chaotic systems are not random; often they are confined to a bounded, lower dimensional subset of phase space, a \textbf{strange attractor}, towards which all close trajectories get asymptotically attracted but without having a periodic motion \footnote{In general if a map contracts volumes in phase space it is called dissipative. A physical system with friction is an example. In contrast, conservative systems cannot have strange attractors because attractors attract all trajectories in a small open set containing it, which contracts the phase space volume.}.
%\footnote{The 'strangeness' partly comes from the fact that trajectories diverge exponentially but remain confined in a zero-volume subset of the phase space. The explanation for this is that they diverge in the direction parallel to the attractor and get attracted in the direction orthogonal to it. GGG}.
These attractors often possess a complex, self-similar fractal structure of non-integer dimension. A common measure is the \textbf{correlation dimension ($D_2$)}. It is based on the correlation integral, $C(\epsilon)$, which measures the probability that two points on the attractor are closer than a distance $\epsilon$. For small $\epsilon$, the correlation integral scales as a power law:
\begin{equation}
    C(\epsilon) = \lim_{N \to \infty} \frac{2}{N(N-1)} \sum_{i<j} \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|) \sim \epsilon^{D_2}
\end{equation}
where $N$ is the number of points on the trajectory and $\Theta$ is the Heaviside step function. The dimension $D_2$ is then found as the slope of the linear region in a log-log plot of $C(\epsilon)$ versus $\epsilon$.

Another powerful tool for analyzing dynamical systems is \textbf{Recurrence Analysis} \cite{MARWAN2007237}. A Recurrence Plot (RP) visualizes the times at which a trajectory revisits a previously visited neighborhood in phase space, within a threshold $\epsilon$. The recurrence matrix is defined as:
\begin{equation}
    % R_{i,j}(\epsilon) = \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|), \quad i,j = 1, \dots, N
    R_{i,j}(\epsilon) = \Theta(\epsilon - \text{Distance}(\mathbf{x}_i, \mathbf{x}_j)), \quad i,j = 1, \dots, N 
\end{equation}
% GGG: check equation 
The visual patterns in a RP provide deep insight: periodic systems produce long, parallel diagonal lines, random systems produce a uniform, noisy plot, and chaotic systems produce a complex geometric structure with short, interrupted diagonal lines. These structures can be quantified using Recurrence Quantification Analysis (RQA).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/rp_periodic_lorenz_stoch.png}
    \caption{Exemplary recurrence plots of (A) a periodic motion with one frequency, (B) the chaotic Lorenz system, and (C) of normally distributed white noise. Figure taken from \cite{DONNER_2011}}
    \label{fig:rp_examples}
\end{figure}

% G: I would like to generally describe the necessary conditions for chaos. (I dont mean SIC and exponential divergence, but rather the underlying characteristics of the system which will make it be chaotic). Then I'd like to explain which components of LLM architecture correspond to these. example: nonlinear coupling and attention

In general, the following structural conditions of a system cause it to be chaotic:
Nonlinearity: % GGG improve paragraph
Stretching and Folding mechanism: Stretching separates close trajectories, causing SIC. this is quantified by the lyapunov exponent. folding confines them to a bounded subset of the state space, a strange attractor.
% maybe "In LLMs, we hypothesize that the self-attention mechanism provides the 'stretching' by amplifying small differences in token representations, while normalization layers and the bounded nature of activation functions provide the 'folding', ensuring the system's states remain within a finite volume."
\cite{Hnon1976ATM}
\cite{ROSSLER1976}
\cite{strogatz_textbook}


\subsection{Large Language Models}
\label{subsec:llms}
Large Language Models are a class of deep neural networks, typically using the Transformer architecture. They are trained on massive text data to predict the probability distribution for the next token in a sequence, and used autoregressively to generate a text output sequentially by sampling from that distribution.

The process begins with tokenization, where input text is broken down into a sequence of integers (tokens) \footnote{The tokens represent fragments of text e.g: "hello", "\textbackslash n", "**", "1", "token", "ization", " ". Tokenization is learned by the model during training.} Each token is then mapped to a high-dimensional vector, its embedding. These embeddings are processed through a stack of Transformer layers. Each layer is primarily composed of two sub-modules: a multi-head self-attention mechanism and a position-wise feed-forward network called Multi-Layer Perceptron (MLP). The self-attention mechanism is the key source of nonlinearity and contextual understanding, allowing the model to dynamically weigh the importance of all other tokens in the context window, and then update the representation of a single token. It can be expressed as \cite{attention}:
% G: somewhere I could be more precise on what first layer and last layer refers to and entails, so that later in results it is more clear. maybe in method explain that i extract the hidden states after each transformers 

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where $Q$ (Query), $K$ (Key), and $V$ (Value) are matrices derived from the input embeddings. The Query represents the current token's focus, the Key represents the information other tokens have, and the Value contains the content of those tokens. The attention mechanism computes a weighted sum of the Values, where the weights are determined by the similarity between the Query and Keys \footnote{As an intuition, attention is like a learned differentiable lookup table. Given a hidden state, it generates a query and returns a weighted sum of values based on the key-query similarity, which is used to update the hidden state. The queries, keys and values are generated from the hidden states in the context window using Q, K and V}. This updated representation becomes the new hidden state for the token. Hidden state refers to the vectors that go through the model's layers \footnote{Embeddings are static, initial vector representations of tokens, while hidden states are dynamic, context-dependent vectors that change as the model processes information through its layers. Embeddings provide an initial meaning for a token, whereas hidden states carry the evolving context, including information from previous tokens in the sequence.}.
In LLMs causal masking is used to ensure that a token is only influenced by previous (and not future) tokens during attention 
\footnote{Nowadays most LLMs are decoder-only and it is necessary to use causal masking for autoregressive generation. Bidirectional (no causal masking) attention can be used in the encoder of a transformer-based model, such as BERT (encoder only) or some translators (encoder-decoder).}.
%\footnote{This is necessary in order to use LLMs autoregressively. Other transformer-based models like}.
Due to the compute cost of attention, which grows with the square of the sequence length, attention is only performed on a sliding context window \footnote{Attention is performed on the minimum value between the sequence length and the context window size.} with the latest token at its right endpoint.
This operation creates a nonlinear feedback and coupling mechanism between all elements of the sequence. The MLP further processes these representations through additional nonlinear transformations. Normalization layers like LayerNorm and RMSNorm normalize the hidden states and project onto a lower-dimensional ellipsoid. This is a bounding operation, preventing state vectors from growing infinitely and thus providing the "folding" part of the "stretching and folding" mechanism required for chaos. This is explained in more detail in the Appendix \ref{subsec:layer_norm}. % GGG revise 
% The output of the final layer is a sequence of hidden states, which represent the contextualized meaning of each token. % G: clarify that for all layers we can get it and also connect it better to how the logits are obtained

During inference, the LLM generates tokens autoregressively, which can be viewed as a discrete-time dynamical system:
\begin{equation}
    % \mathbf{s}_{t+1} = f(\mathbf{s}_t, \mathbf{s}_{t-1}, ..., \mathbf{s}_0) % G: maybe it should be \mathbf{s}_{t-context_window_size} to be more precise
     s_{t+1} = f(s_t, s_{t-1}, ..., s_{t-Context_size+1})
\end{equation}
where $\mathbf{s}_t$ represents the hidden state at step $t$. 
% where $\mathbf{s}_t$ represents the model's internal state (the sequence of hidden states) at step $t$. % I wouldn't say "state" because state means that it has all the info necessary to make the next step, but here you need also the context 
To generate the next token, the hidden state of the last token is passed through a final linear layer, the unembedding, to produce logits over the entire vocabulary, which are then converted to a probability distribution via a softmax function. A decoding strategy is then used to select a token. \textbf{Greedy sampling} deterministically selects the token with the highest probability. In contrast, \textbf{probabilistic sampling} introduces stochasticity, often controlled by a temperature parameter ($T$). Higher temperatures flatten the distribution, making the output more diverse.



\subsection{Analogy and Limitations}
\label{subsec:analogy_limitations}
We propose viewing the autoregressive process of an LLM as a high-dimensional, discrete-time dynamical system. The sequence of hidden states serves as the trajectory evolving in the model's embedding space. The self-attention mechanism acts as a (state-dependent) nonlinear coupling between different tokens. 
% Each token's representation is affected by (previous) other tokens, with softmax, ReLU, layer norm providing the nonlinearity.

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Chaotic System} & \textbf{LLMs} \\ \midrule
Phase Space & Embedding / Hidden State Space \\
State Vector & Hidden State Vector \footnote{Formally, the state should describe all the information necessary to predict the next step, which in LLMs includes all the hidden states in the context window. This is handled by expanding the phase space to include those as well growing up to $\mathbb{R}^{D \times \text{context\_size}}$. The equivalent in chaos theory is a delay system where the phase space can be infinite-dimensional. We believe this makes the discussion unnecessarily less clear, so the reader should keep this in mind, but we will not speak in this terms. We will return to this point when discussing methods to calculate distance between trajectories using sliding windows.} \\ %  todo: this footnote is not appearing in the pdf, i suspect it is because i t is inside a table. fix
Phase Space Trajectory & Sequence of Token Text or Hidden States \\
Continuous Time Evolution & Discrete Token Generation Step \\
Differential Equation & Autoregressive Function \\
Numerical Solution & Forward Pass Inference \\
Nonlinear Coupling & Self-Attention Mechanism \\
Initial Condition & Initial Prompt Text or Embedding \\
Measurement Uncertainty & Floating Point Precision, Quantization \\ \bottomrule
\end{tabular}
\caption{Analogies between concepts in chaos theory and LLMs.}
\label{tab:analogy}
\end{table}
% G: could add stretching and folding. table too big rn
% could add "Stretching" -> "Attention/MLP layers" and "Folding" -> "Normalization layers". 


However, the analogy is not perfect and has critical limitations. LLM dynamics are discrete, not continuous, with a fixed "time step" of one token. The act of sampling a token from the probability distribution to generate the next token can be seen as a "collapse" or a sharp discontinuity in the trajectory's evolution \footnote{The hidden states at the first layer are also discrete and correspond exactly to a column of the embedding matrix. In deeper layers, the possible hidden states are more numerous (and effectively continuous), as they are generated through nonlinear transformations (transformer layers) of the previous hidden representations.}.

% G: can clarify that it matters in that this new token is the starting point for the processing in the 1st layer, but all the previous hidden states remain and will influence at each layer.
LLMs are usually used with probabilistic sampling, which adds a random component and makes it non-deterministic. Furthermore, the dimensionality of the state space is extraordinarily high (thousands of dimensions), which poses significant challenges for traditional chaos analysis techniques.

% G: also in chaotic systems the nonlinear coupling is between variables, but in llm it is between tokens, which would be the state of the system (so all the variables) at different times
% ?? In a classical system like Lorenz, the coupling is between fixed variables (x, y, z). In a Transformer, the coupling is between *positions* in a sequence, and the 'variables' at those positions are entire state vectors. It's a much more complex, higher-order form of coupling. Worth stating this clearly. ??? GGG

% G: as i understand it. MLP provides nonlinearity, attention nonlinear coupling and (maybe) stretching. normalization keeps the system bounded (folding). layer norm is more like projecting to the plane perpendicular to (1,1,1,..,1) plus affine rescale.

% G: is normalization folding or stretching? normalize would siggest constraining so folding, but attention is cross communication so also folding. talk this out and dicuss. which processes in LLM constrain and maintain boudedness? is this folding? which stretch= is stretching individual?
%  Normalization is probably folding/constraining. Attention is primarily stretching. attention allows one state vector to be influenced by many others. If a small change in one vector causes it to attend more strongly to a very different vector, its representation can be 'stretched' dramatically towards the VALUE (from K-Q) other vector. The MLP layers, with their non-linear activations, also contribute to stretching. The folding comes from normalization layers and activation functions like tanh that are inherently bounded.

\section{Methods}
\label{sec:methods}

\subsection{Trajectory Distance Metrics}
\label{subsec:trajectory_distance_metrics}
We use a suite of metrics to quantify the divergence between pairs of trajectories, $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n)$ and $\mathbf{Y} = (\mathbf{y}_1, ..., \mathbf{y}_m)$. Distance between vector representations of text encode meaning about its semantic similarity \cite{text2vec} \cite{reimers2019sentencebertsentenceembeddingsusing} .
% G: cite word2vec and some paper with clustering from LLM or show my clustering. also some from text2vec embedders
% G: also mention that some metrics are based on other metrics: like rank eigen uses cos similarity to compare pairs of vectors. check which use L2
% We justify the use of cosine similarity as it is a meaningful metric both in the hidden state and embedded text space.
% "Since there is no single canonical distance metric for high-dimensional semantic spaces, we employ a diverse suite of metrics. This ensures our findings are robust and not an artifact of a particular choice of measurement. Each metric captures a different aspect of trajectory divergence, from pointwise orientation (Cosine) to global shape (Fréchet, Hausdorff)."

\subsubsection{Vector-wise Metrics}
\label{sssec:methods_vector_metrics}
These metrics are applied pointwise between corresponding vectors of two trajectories, yielding a time series of distance values.
\begin{itemize}
    \item \textbf{Cosine Distance:} Measures the angle between two vectors, capturing their orientation similarity. It is widely used to compare similarity between vector embeddings \footnote{If all vectors are normalized, cosine distance and Euclidean distance are monotonically related, so they produce the same relative ordering of pairwise similarities even though their numerical values differ. Empirically the results obtained are very similar.}. It is defined as $1 - \text{Cosine Similarity}$:
    \begin{equation}
        d_{cos}(\mathbf{a}, \mathbf{b}) = 1 - \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}
    \end{equation}
    \item \textbf{Normalized Cross-Correlation:} Measures the similarity between two vectors as a function of the displacement of one relative to the other. For discrete vectors at zero lag, it is related to the cosine similarity. (GGG improve use formula)
\end{itemize}

\subsubsection{Trajectory-wise Metrics}
\label{sssec:methods_trajectory_metrics}
These metrics yield a single scalar value for the distance between two entire trajectories. To obtain a divergence curve, we apply these metrics in a sliding window of increasing size. For these, we used Euclidean distance within each.
% G: maybe explain how using sliding window is like saying that the state of the system is not described by a single hidden state (or token), but by the context too (similar too delay systems)
% "The use of a sliding window is motivated by the fact that an LLM's state depends on a history of previous states (the context window). By comparing windows of states, we are approximating the distance in a reconstructed phase space, analogous to the method of time-delay embeddings used for classical chaotic systems."
\begin{itemize}
    \item \textbf{Dynamic Time Warping (DTW) \cite{SalvadorChan2007}:} Finds an optimal non-linear alignment between two sequences. It computes a cost matrix where $C_{i,j}$ is the distance between $\mathbf{x}_i$ and $\mathbf{y}_j$. The DTW distance is the value in the final cell of an accumulated cost matrix $D$, defined by the recurrence:
    \begin{equation}
        D_{i,j} = C_{i,j} + \min(D_{i-1, j}, D_{i, j-1}, D_{i-1, j-1})
    \end{equation}
    \item \textbf{Fréchet Distance \cite{Denaxas2023} \cite{EiterMannila1994}:} Intuitively, the minimum length of a leash needed to connect a person and a dog walking along their respective paths. It is sensitive to the ordering of points. For two curves $P$ and $Q$, it is:
        \begin{equation}
            d_F(P, Q) = \min_{\text{couplings}} \max_{(i, j) \in \text{path}} \| P_i - Q_j \|
        \end{equation}
        In this work, we use the discrete Fréchet distance, using monotonic alignments of the sampled points, respecting their order but without continuous reparameterization.
        % G: it is not very clear. 
        %G: using monotonic alignments -> which finds the minimal leash length over all monotonic alignments of the sampled points (discrete Fréchet).
    \item \textbf{Hausdorff Distance \cite{SciPyDirectedHausdorff}:} Measures the greatest of all distances from a point in one set to the closest point in the other set. It is a pure set-based distance.
    \begin{equation}
        d_H(X, Y) = \max \left( \sup_{\mathbf{x} \in X} \inf_{\mathbf{y} \in Y} \|\mathbf{x}-\mathbf{y}\|, \sup_{\mathbf{y} \in Y} \inf_{\mathbf{x} \in X} \|\mathbf{x}-\mathbf{y}\| \right)
    \end{equation}
    % implementation computes the symmetric Hausdorff as max(directed_hausdorff(a,b), directed_hausdorff(b,a)) — that is standard.
    % For sliding-window analysis, hausdorff.py does not compute a strict per-window Hausdorff in all cases but computes per-point nearest distances and then aggregates those distances by either "max_of_mean" (default) or "mean_of_max" (configurable). This is a deliberate pragmatic choice;  it changes sensitivity (averaging vs extreme) and makes the sliding-window Hausdorff different from the textbook per-window Hausdorff if you choose mean-based aggregation.
    \item \textbf{Principal Component Dissimilarity (PCD):} We also defined a new distance metric based on the structural similarity of two trajectories. This metric compares the geometric structure of two trajectories by analyzing the alignment of their principal components (PCs). For each PC in the first trajectory, it finds the most similar PC in the second. The dissimilarity is then calculated as the total cosine distance between these matched pairs of principal components. This is explained in detail in \ref{subsec:pca_distance}.

    % G: wasserstein not included. but maybe i wont use it
\end{itemize}



\subsection{Recurrence Analysis}
\label{subsec:recurrence_analysis}
We generate Recurrence Plots (RPs) using the self-similarity matrix of a single hidden state trajectory with all the distance metrics (GGG "all distance metrics). We analyze the plots qualitatively and quantitatively using Recurrence Quantification Analysis (RQA) metrics, including:
\begin{itemize}
    \item \textbf{Determinism (DET):} The percentage of recurrence points forming diagonal lines, indicating predictability.
    \item \textbf{Laminarity (LAM):} The percentage of recurrence points forming vertical lines, indicating periods of stable or "laminar" states.
    \item \textbf{Entropy (ENT):} The Shannon entropy of the distribution of diagonal line lengths, reflecting the complexity of the deterministic structure.
\end{itemize} % G: include only the ones I actually use

% G: if i could say something about choice of threshold that would be great. for the RP images not so necessary but in RQA part yes

\subsection{Dimensionality}
\label{subsec:dimensionality_results}
We estimate the correlation dimension ($D_2$) of the trajectories using the Grassberger-Procaccia algorithm \cite{GRASSBERGER1983189} \footnote{Other methods such as box dimension or Hausdorff dimension are less robust and harder to compute in higher dimensions}. This technique can be used to distinguish between deterministic chaos and random noise, and it is closely related to the fractal and information dimension. This can be calculated from the self-similarity matrix by applying different thresholds (which correspond to the distance $\epsilon$). 
% G: expand. mention that when T != 0, the fits are bad, but T = 0 (at last layer), fit is good. correlation dimension can be used to identify  deterministic (chaotic) systems. I need to add more plots


% G: At the first layer, the curve is not as continuous, which can be explained by the fact that the tokens in the first layer are discrete and taken from the embedding matrix. the first layer is dicrete vectors from vocab, not continuous manifold. 



% \subsection{Clustering}
% \label{subsec:clustering_methods}
% Furthermore, we apply clustering algorithms to the set of hidden state vectors within a trajectory to identify distinct operational states or regions in the model's high-dimensional phase space.
% % G: probably eliminate




\subsection{Experiments}
\label{subsec:experiments}
% G: conretely explain what I did: model choice, params, dim, fix length generation, context window, etc...

For this study, we used the following experimental setup:
\begin{itemize}
    \item \textbf{Model:} deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \cite{deepseek2025r1distillqwen1.5b} \footnote{The model used is distilled from DeepSeek R1 reducing the number of parameters from 671 billion to 1.5 billion. It is trained and fine-tuned to replicate the behavior from the DeepSeek R1. This distilled model is based on the Qwen2.5 family and there are differences in the architecture, which are beyond the scope of this study. All the points mentioned in Section \ref{subsec:llms} apply to both, and generally to most LLMs. The reason we used this model was memory limitations in the GPUs available for this study.}
    \item \textbf{Temperature:} 0 (to ensure determinism)
    \item \textbf{Context Window:} 3096 tokens
    \item \textbf{Initial Perturbation Radius:} 0.0003 - 0.0004 % GGG explain how we got this: tuning until the responses were similar but not identical and we got nice divergence curves
    \item \textbf{Vocabulary Size:} 151936
    \item \textbf{Hidden/Embedding Dimension:} 1536
    \item \textbf{Attention Heads:} 12
    \item \textbf{Hidden Layers:} 28
\end{itemize}

We use the model for inference (text generation) on a Tesla T4 GPU, and we obtain the generated tokens, text and hidden states at all the layers, i.e., the representations after each transformer block \footnote{The shape of the complete hidden state tensor for a given response is (number of layers, number of tokens, hidden dimension)}. A particular feature of the model used, DeepSeek R1, is that it uses Chain-of-Thought reasoning at the start of every answer \cite{deepseekR1}.

\subsubsection{Initial Condition Generation}
\label{subsec:init_cond_gen}
To test for sensitivity to initial conditions, we generated a set of trajectories from close starting points. We begin with a single prompt, tokenize it, and retrieve its initial embedding tensor $\mathbf{E}_0 \in \mathbb{R}^{L \times D}$, where $L$ is the prompt length and $D$ is the model's hidden dimension. We then create a "blob" of $N$ perturbed initial conditions by adding a small, random direction perturbation tensor $\in \mathbb{R}^{L \times D}$ of fixed magnitude (radius $r$). Then we use LLM to continue each trajectory.
% GGG mention different prompts

\section{Results}
\label{sec:results}

\subsection{Trajectory Divergence and Lyapunov} \label{res:lyapunov} % GGG 

There were some challenges inherent to the system under study. In any bounded (GGG) system, there is a maximum possible value for the distance, and the distance between two diverging trajectories will eventually reach the maximum and saturate. In our case, especially when using sentence embeddings, the number of data points until the saturation region was often small (GGG around 6), making any type of curve fitting difficult.
% G: bounded because vectors are normalized

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/cross_corr_pearson_0_1.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/dtw_timeseries_0_1.png}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/frechet_timeseries_0_1.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/hausdorff_timeseries_0_1.png}
    \end{subfigure}
    \caption{Divergence between pairs of trajectories.}
    \label{fig:distance_hidden}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/cross_corr_pearson_0_1.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/dtw_timeseries_0_1.png}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/frechet_timeseries_0_1.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_1.png}
    \end{subfigure}
    \caption{Divergence between pairs of trajectories.}
    \label{fig:distance_embed}
\end{figure}

We see that the trajectories remain extremely close for some steps until they suddenly diverge (GGG). So the initial perturbation remains latent in the hidden states, but does not express itself immediately in the tokens until a certain step, where the sampled token by two trajectories is different, which then leads them to quickly diverge.
% it suggests a "critical point" or "bifurcation" where the latent divergence crosses a threshold and manifests as a macroscopic change in the output token. kinda

\subsection{RP}
\label{subsec:rp_results}

Strong similarities can be seen between the RP from the Lorenz \cite{DeterministicNonperiodicFlow} system as seen in Fig. \ref{fig:rp_examples} and from the LLMs output. % GGG
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/interstellar_propulsion_review_recurrence_first_thr_0.3.png}
        \caption{Interstellar propulsion, first layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/interstellar_propulsion_review_recurrence_last_thr_0.3.png}
        \caption{Interstellar propulsion, last layer}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/quantum_consciousness_hallucination_recurrence_first_thr_0.3.png}
        \caption{Quantum consciousness, first layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/quantum_consciousness_hallucination_recurrence_last_thr_0.3.png}
        \caption{Quantum consciousness, last layer}
    \end{subfigure}
    \caption{Recurrence plots for different prompts and layers.}
    \label{fig:rp_comparison_layers}           
\end{figure}

The Chain-of-Thought reasoning block, the first tokens generated by the model, can be identified in the RP (left bottom).
% it suggests the model enters a specific, stable 'reasoning state' before beginning its main output. This is a great example of using RPs to identify functional stages in the generation process. 

% G: not always at the last layer, but also middle layers are quite chaotic
Chaotic features are more strongly present at the last layers of the model as showed in the RP \ref{fig:rp_comparison_layers} and pointwise dimension \ref{fig:dim_first_last}. This is due to the progressive addition of more nonlinearity and coupling at each layer. As pointwise dimension is calculated from the RP, it is sensible that they show the same result.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.04/traj_2_recurrence_thr_0.3.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.04/traj_3_recurrence_thr_0.3.png}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.02/traj_1_recurrence_thr_0.3.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.02/traj_6_recurrence_thr_0.3.png}
    \end{subfigure}
    \caption{Examples of recurrence plots obtained for different prompts.}
    \label{fig:rp_interesting}
\end{figure}

\subsection{Dimensionality}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_first_first.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_last_last.png}
        \caption{Last layer}
    \end{subfigure}
    \\[0.5em]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/quantum_consciousness_hallucination/cosine_sim_first_first.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/quantum_consciousness_hallucination/cosine_sim_last_last.png}
        \caption{Last layer}
    \end{subfigure}
    \caption{Correlation dimension plots for the first and last layers of the LLM. The top and bottom rows are from different prompts}
    \label{fig:dim_first_last}
\end{figure}

% random at first layer is because it doesn't have all the info of the context so it appears random.
% only at last layers can you see the true chaoicity
%  A non-integer value for Dimenion is a hallmark of a fractal strange attractor. 



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_first_first_dist.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_last_last_dist.png}
        \caption{Last layer}
    \end{subfigure}
    \\[0.5em]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/quantum_consciousness_hallucination/cosine_sim_first_first_dist.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/quantum_consciousness_hallucination/cosine_sim_last_last_dist.png}
        \caption{Last layer}
    \end{subfigure}
    \caption{Distribution of cosine distance for the first and last layers of the LLM. The top and bottom rows are from different prompts}
    \label{fig:distribution_cos_dist}
\end{figure}

% In the cases investigated the correlation dimension obtained using the L2 distance metric was aproximately twice the one obtained using cosine similarity.

The measured dimension value obtained varied significantly depending on the distance metric used, % GGG
while in every case, the Correlation Dimension vs Threshold curve was similar, but with a different slope. This leads


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/dimension/random2.png}
    \caption{Correlation Dimension obtained using 100k random vectors of dimension 1536 (same as hidden dimension)}
    \label{fig:random_vectors_dimension}
\end{figure}

% G: it would be great to show if last layers have a lower dimension, which could imply more "compression" of the data
% G: try to relate the dimension here with information compression, which is something super important that LLMs do.  

\section{Conclusions and Discussion}
\label{sec:conclusions}

% \subsection{Possible Explanations for LLMs being chaotic}

% (rough) invariance for different metrics {hausdorf, frechet, dtw} (and also embedders). discuss. they literally look the same. I mean to be fair the data is the same. quite trivial
% It's a sign of robustness. These metrics measure very different geometric properties. The fact that they all show the same qualitative behavior is evidence that we are observing a real dynamical phenomenon, not an artifact of your measurement tool.

This study was limited in studying a single LLM model, while we expect interesting results if this was done with other models and architectures. Certain architectural features of the model, such as the Chain-of-Thought reasoning could be recognized in the recurrence plots.

% mention the challenges and successes with high dim data. also distance metrics. and the ones I invented
% "Our results paint a consistent picture of LLM dynamics. We observe sensitive dependence on initial conditions, characteristic of chaos. Recurrence plots reveal complex, deterministic structures that evolve through the network's layers, culminating in chaotic-like behavior in the final layers. Furthermore, dimensionality analysis suggests these complex dynamics unfold on a low-dimensional fractal attractor. This suggests that LLMs operate at the 'edge of chaos', leveraging these rich dynamics to balance creativity and coherence. We propose that the interplay between the 'stretching' of the attention mechanism and the 'folding' of normalization layers is the fundamental architectural driver of this behavior."



% G: Try to frame some of the challenges and difficulties as interesting things
% - finding a good distance metric is tricky. it seems like each token (and its hidden state) is representing mainly that token's meaning, and not the whole state of the system (or trajectory) up until that point. In order to really describe the state of the system, we need the previous tokens too (like in the case of a delay system in chaos and nonlinear systems). The usage of sliding windows and/or sentence embedder for the distance metrics does something like this (capturing local context), and the fact that it makes the distance curves much better shows how important the context is
% this point is quite imporatnt for divergence, but not so necessary (or convenient) for RP
% empirically from the data analysis the LLM is better modeled as a delay system. The 'state' is not just the current hidden vector, but a window of past vectors. This justifies sliding window approach and connects LLM dynamics to a well-understood class of complex systems.

\section{Acknowledgments }
\label{sec:acknowledgements}

I would like to thank my supervisors Dr. János Török and Kristóf Benedek for their help and support during this work.

\newpage
\bibliographystyle{unsrt}
\bibliography{lit}

\section{Appendix}
\label{sec:appendix}

\subsection{Intuition behind different metrics}
\label{subsec:appendix_intuition_metrics}

In order to gain a more intuitive understanding of each metric (GGG and maybe embedding too), we show RP from the same trajectory.

% G: maybe use cross_corr instead of cross_cos as cross_corr is bettter in distance timeseries
\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/hausdorff_traj0.png}
        \caption{Hausdorff}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/dtw_fast_traj0.png}
        \caption{DTW}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/frechet_traj0.png}
        \caption{Fréchet}
    \end{subfigure}
    % Second row
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/rank_eigen_traj0.png}
        \caption{PCD}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/cos_traj0.png}
        \caption{Cosine similarity}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/cross_cos_traj0.png}
        \caption{Cross-cosine similarity}
    \end{subfigure}
    \caption{Distance matrices for a single trajectory using different metrics and using the hidden states.}
    \label{fig:distance_metrics_comparison_hidden}
\end{figure}

\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/hausdorff_traj0.png}
        \caption{Hausdorff}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/dtw_fast_traj0.png}
        \caption{DTW}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/frechet_traj0.png}
        \caption{Fréchet}
    \end{subfigure}
    % Second row
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/rank_eigen_traj0.png}
        \caption{PCD}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/cos_traj0.png}
        \caption{Cosine similarity}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/cross_cos_traj0.png}
        \caption{Cross-cosine similarity}
    \end{subfigure}
    \caption{Distance matrices for a single trajectory using different metrics and using the text embeddings.}
    \label{fig:distance_metrics_comparison_embed}
\end{figure}

% G: Here i want to show how embeddings kind of do some averaging and pooling and context, and how some metrics pick up different features, some more global some more detail. 
% G: mention window size


\subsection{Use of Sliding Window and Sentence Embeddings}
\label{subsec:sliding_window}

\begin{figure}[H]
    \centering
    % First row: Hidden state space
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_1.png}
        \caption{Hidden state, window size 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_8.png}
        \caption{Hidden state, window size 8}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_16.png}
        \caption{Hidden state, window size 16}
    \end{subfigure}
    % Second row: Sentence embedding space
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_1.png}
        \caption{Embedding, window size 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_8.png}
        \caption{Embedding, window size 8}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_16.png}
        \caption{Embedding, window size 16}
    \end{subfigure}
    \caption{Divergence between two trajectories using Hausdorff distance, for different sliding window sizes. Top row: hidden state space. Bottom row: sentence embedding space.}
    \label{fig:sliding_window_compare}
\end{figure}

We see that in the hidden state space, without a sliding window, the distance measurement is very noisy, which is not as pronounced when using the sentence embedders. This shows that the hidden states represent mainly the meaning of the token they are associated with, and not so much of the context (even though they have access to information from other tokens via attention). 
We were originally expecting the hidden states to represent the whole state of the system up to that point, but this turned out to be false, and considering the hidden states in the context window is also necessary. This makes LLMs more similar to a delay system. % G: 
% G: With window size 1, the hidden state distance is noisy. As you increase the window size, a clear divergence curve emerges. This visually demonstrates that the 'true' state of the system is a sequence of vectors, not a single vector. connect size of window with semantic


\subsection{Handling outliers} % G: if this section is brief, I can include it directly in each subsection in results/methods
\label{subsec:appendix_handling_outliers}

In section \ref{res:lyapunov} there were some initially nearby trajectories which remained close to each other and did not diverge, having the same text output and almost identical hidden states. This represented approximately GGG 10\% of the pairs of trajectories.
% maybe not an outlier, but a result! It suggests the existence of stable regions or fixed points in the state space. Even in a chaotic system, not all initial conditions lead to divergence. Some might lie on stable manifolds. The fact that ~10% of trajectories are stable is a quantifiable measure of the system's stability. 
% G: i kinda estimated the 10% number. and also it could be the fault of using random direction perturbation and two of them being close randomly

\subsection{Dimensionality of the LLM vocabulary}
\label{subsec:appendix_vocab_dim}


We also did this analysis on the embedding and unembedding matrix, which for the LLM model we use is the same. This matrix represents the vocabulary of the model, and it is not a trajectory. 
The reason we did this is because of the large number of data points available (around 150 thousand) without requiring any computation, and that it could provide insights into the structure of the vocabulary space. It is interesting to see that the results are different from those obtained from the analysis done on the trajectories, or what is obtained from a random set of vectors. The value obtained for the correlation dimension is smaller than expected.
% G: I don't have a good explanation or much to say here.
% G: a trajectory generated by the model is not just a random walk through this vocabulary space; the trajectory uses new vectors

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/vocab/vocab_full.png}
        \caption{Correlation Dimension = 1.2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/vocab/vocab_last.png}
        \caption{Correlation Dimension = 5.4}
    \end{subfigure}
    \caption{Correlation dimension of the LLM's embedding matrix.}
    \label{fig:embedding_dim}
\end{figure}


\subsection{Layer normalization (G: ignore for now, this is bad)}\label{subsec:layer_norm}% G: this is quite bad currently, I'll make it shorter and more relevant.
% G: mention other normalizations? like batch?

Layer normalization (LayerNorm) acts on a single hidden state vector \(h\in\mathbb{R}^d\) (e.g. a token embedding) by (i) mean-centering, (ii) variance normalization, and (iii) a learned affine anisotropic rescaling.
\begin{align}
\mu(h) &\;=\; \frac{1}{d}\sum_{i=1}^d h_i, \\
\sigma^2(h) &\;=\; \frac{1}{d}\sum_{i=1}^d (h_i-\mu(h))^2, \\
\widehat{h} &\;=\; \frac{h - \mu(h)\mathbf{1}}{\sqrt{\sigma^2(h)+\varepsilon}}, \label{eq:ln_norm}\\
y &\;=\; \gamma \odot \widehat{h} + \beta \;=\; D\widehat{h} + \beta, \label{eq:ln_affine}
\end{align}
where \(\mathbf{1}=(1,\dots,1)^\top , \gamma, \beta \in\mathbb{R}^d\), and \(D=\operatorname{diag}(\gamma)\).
% G: pretty unreadable. 
% G: benefits:
% Introducing a normalization of the hidden states after every operation as part of the model architecture made th training between 4 and 20 times faster \cite{loshchilov2025ngptnormalizedtransformerrepresentation}
% Representation learning on the hypersphere leads to more stable training, greater embedding space separability, and better performance on downstream tasks \cite{wang2022understandingcontrastiverepresentationlearning}
% Normalization techniques are beneficial \cite{salimans2016weightnormalizationsimplereparameterization}

% G: maybe we can hypothesize whether norm keeps the model more stable by limiting lyapunov exponents and keeping it at the edge of chaos

\begin{itemize}
  \item \(h\in\mathbb{R}^d\): input hidden state (one token, one sample).
  \item \(\mu(h)\): coordinate mean of \(h\)
  \item \(\sigma^2(h)\): empirical coordinate variance of \(h\)
  %\item \(\widehat{h}\): normalized (zero-mean, unit-variance) vector
  \item \(\gamma\in\mathbb{R}^d\) (or \(D=\mathrm{diag}(\gamma)\in\mathbb{R}^{d\times d}\)): per-coordinate learned scale (anisotropic linear part)
  \item \(\beta\in\mathbb{R}^d\): learned bias
  \item \(\varepsilon\): numerical stabilizer
\end{itemize}

Geometrically, LayerNorm performs a series of transformations. First, the centering step projects the hidden state onto a hyperplane orthogonal to the vector $\mathbf{1}$, effectively removing the mean component. The subsequent variance normalization is a nonlinear operation that maps the centered vector to a point on a sphere of radius $\sqrt{d}$. This step reduces the degrees of freedom from $d$ to $d-2$, as the image lies on a $(d-2)$-sphere within the mean-zero hyperplane. Finally, the affine transformation scales the sphere into an ellipsoid and translates it, without mixing the coordinate axes.

\paragraph{Jacobian (local linearization)}
Let \(u(h):=h-\mu(h)\mathbf1\) and \(\|u\|=\sqrt{u^\top u}\).  A convenient form of the Jacobian of the normalization map \(\widehat{h}(h)\) is
\[
J_{\widehat{h}}(h)
\;=\;
\frac{\sqrt{d}}{\|u\|}\,\Big(I - \frac{u u^\top}{\|u\|^2}\Big)\Big(I - \frac{1}{d}\mathbf1\mathbf1^\top\Big),
\]
and the full LayerNorm Jacobian (including \(D\)) is \(J_y(h)=D\,J_{\widehat{h}}(h)\).  For generic \(h\) (with \(d>2\)) this linearization has rank \(d-2\): two null directions correspond to the mean direction \(\mathbf1\) and the radial direction \(u\) (the latter is collapsed by the radial normalization).
% G: By killing the �� u-direction, LayerNorm throws away “how much” the features deviate from the mean and keeps only “in what relative proportions” they deviate.
% G: could emphasize that the radial direction u is specific for every h, but the mean centering is global.

\paragraph{Interpretation for dynamics} % GGG \paragraph
% \label{par:interpretation_dynamics}
LayerNorm is therefore a structured \emph{many-to-one} nonlinear map: it (i) removes one linear degree of freedom (the mean), (ii) collapses the radial coordinate in the centered hyperplane (folding onto a compact manifold), and (iii) applies an anisotropic linear rescaling and shift.
% G: (ii) is too much
% In dynamical terms this provides a strong folding/bounding operation; any stretching/expansion must come from the other components (linear layers, attention, pointwise nonlinearities) for the overall layer-to-layer map to exhibit expansion+folding behavior.
% G: I'm not sure if this is sound. why call it a folding when it is rather a projection
% G: It's both. A projection is a form of folding. It takes a larger space and maps it to a smaller one. In this case, it's a projection onto a hypersphere, which is a bounded manifold. This  acts as a folding mechanism, preventing trajectories from escaping to infinity. 

\subsection{Principal Component Dissimilarity (PCD)}
\label{subsec:pca_distance} 

% We also defined a new distance metric based on the structural similarity of two trajectories. It showed similar results to the other metrics but it was noisier. GGG repeated

We perform a PCA on each trajectory obtaining the principal directions (eigenvectors $V^{(i)}$ of the centered data covariance matrices) for each trajectory. To compare the two sets of eigenvectors, we compute the cosine similarity matrix $S$.
For each eigenvector of the first trajectory $v^{(1)}_a$, we find the index $b^*$ of the most similar eigenvector in the second trajectory:
\[
b^* = \arg\max_b S_{ab}
\]
We can show the ranking of eigenvectors to show how similar two trajectories are. The closer to the proportionality line, the more similar two trajectories.

To get a distance scalar, we sum the cosine distances for these matched pairs \footnote{We also tried calculating the root mean square deviation with respect to the proportionality line but it was a worse metric GGG}:
\[
D_{\text{sum\_cos\_dist}} = \sum_{a=1}^k \left[1 - S_{a b^*}\right]
\]
where $D_{\text{sum\_cos\_dist}}$ is the scalar metric quantifying the dissimilarity between the two trajectories.

This metric is not symmetric. Matching is performed from the eigenvectors of the first trajectory to those of the second, and the assignment is not necessarily reciprocal. We compute both and average the two.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{plots/rank_eigen/rank_eigen_pca_0_2.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{plots/rank_eigen/rank_eigen_pca_0_1.png}
    \end{subfigure}
    \caption{Principal Component eigenvector ranking between trajectories. The left image shows two more similar trajectories} % GGG
    \label{fig:rank_eigen}
\end{figure}


\subsection{Ideas that Failed}
\label{subsec:appendix_failed_ideas} % G: under development. maybe wont be in paper
In this section, we quickly mention a few ideas that did not work to prevent others from making the same mistake.

We also investigated the effect of temperature

We also used the Wasserstein metric for the data analysis but it was noisy and not very informative.

We also used cross-recurrence plots to compare two trajectories, but found it not so clear. To compare different trajectories we preferred the methods explained in the paper.

Inspired by \cite{DONNER_2011} and \cite{Donner_2010} \cite{ZOU20191} we did the recurrence analysis using complex networks, which comes naturally by considering the recurrence matrix as the adjacency matrix of the network. While interesting, we found the results easier to analyze using RQA than this method. % G: improve

\subsection{Recurrence Plots for different metrics and embeddings} % GGG better name. also i show distance matrix no rp

In order to provide some intuition on the different metrics and on the effect of using raw hidden states or sentence embeddings, we show 


\subsection{Limitations and shortcomings of this study (GGG this won't be in the paper in this form, I just want to keep track of these):}
\label{subsec:appendix_limitations}

- i can't explain what the value of the dimension obtained *means*. if the dimensionality thing just shows that the system is deterministic, then it is quite trivial
- i only qualitatively compared the recurrence plots obtained
- i failed at aggregating results from pairs of trajectories (so far)
- the recurrence plots for different prompts are quite different and not all look chaotic
% G: bug or finding? It suggests the dynamics are prompt-dependent. Some prompts might induce more stable, predictable behavior, while others induce more chaotic, creative behavior. 
- i only did english prompts
- RQA done on different layers does not show enormous differences (although i claim that the last layers are more chaotic than the first layer). specifically
- when i generate the initial conditions I was a bit stupid and just added a fixed-magnitude random-direction perturbation. So I did not really measure the initial distance between trajectories, and i did not really measure it. I should have done a hyper equilateral triangle, but i did a hyper sphere.
- i did not really show a strange attractor, i just hint at it. i could not quantitatively compare the results for different prompts (or even for the same prompt but disturbed) in the RP and dimension stuff. One might ask: if you say that you get a sort of strange attractor, is it the same for different prompts? is there any similarity between them?. My answer would be: i dont know
- relatively short trajectories:
Using a model with so few parameters and having to limit the context window size significantly, it was difficult to make long
Due to memory constraints we had in the GPUs available, there was a tradeoff between the length of the generated text, and the model size and context window. This meant that we either had to do short responses (~3096 tokens long GGG), or if we wanted longer ones, we needed to use a smaller model or a smaller context window, which then caused the output quality to degrade, often resulting in the model getting stuck in a repetitive loop. The fact that the temperature was set to zero did not help.
What we did is use the smallest model and context window size possible while retaining a good quality output text by reading through them \footnote{also recurrence plots at a glance show if a model gets stuck in a repetitive loop}. 
We did not think this repetitive loops were worth exploring, since these are a feature which appears in very basic and small LLMs with a short context window and zero temperature, and state of the art models will overcome this.


\end{document}



\documentclass[a4paper,12pt]{article}
\usepackage{graphicx,amssymb,textcase,fancyhdr,enumerate,wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Use T1 encoding
\usepackage{mathptmx} % Use Times New Roman font
\usepackage[margin=2cm]{geometry} % Set margins to 2 cm
\usepackage{setspace} % For line spacing
\setstretch{1.4} % Set line spacing to 1.4
\usepackage{icomma}
\usepackage{xcolor}
\usepackage{bm}
\usepackage[unicode,colorlinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=green,      
    urlcolor=blue,
}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{mathtools}
\usepackage[normalem]{ulem}
\usepackage{tablefootnote}
\usepackage{subcaption}
\usepackage{amsmath} % for the cases environment
\usepackage{amsfonts} % additional math symbols (if needed)
\usepackage{graphicx}

\usepackage[style=numeric,sorting=none,maxbibnames=3,minbibnames=1]{biblatex}
\addbibresource{lit.bib}

\DeclarePairedDelimiter\abs{|}{|}
\DeclareMathOperator{\tg}{tg}
\definecolor{new_red}{HTML}{ff0000}
\definecolor{new_blue}{HTML}{0000ff}
\definecolor{new_green}{HTML}{009a00}
\definecolor{new_orange}{HTML}{ff8000}

\colorlet{light_red}{new_red!20!white}
\colorlet{light_blue}{new_blue!20!white}
\colorlet{light_green}{new_green!20!white}
\colorlet{light_orange}{new_orange!20!white}
\newcommand{\JT}[1]{{\color{blue} #1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vv}{\vect{v}}
\newcommand{\vu}{\vect{u}}

\newcommand{\hlb}[1]{\mathpalette\highlightb{#1}}
\newcommand{\highlightb}[2]{\colorbox{light_blue}{$#1#2$}}

\newcommand{\hlr}[1]{\mathpalette\highlightr{#1}}
\newcommand{\highlightr}[2]{\colorbox{light_red}{$#1#2$}}

\newcommand{\hlo}[1]{\mathpalette\highlighto{#1}}
\newcommand{\highlighto}[2]{\colorbox{light_orange}{$#1#2$}}

\newcommand{\hlg}[1]{\mathpalette\highlightg{#1}}
\newcommand{\highlightg}[2]{\colorbox{light_green}{$#1#2$}}

\setlength{\parskip}{0pt plus 0pt}
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.1}

\definecolor{myblue}{RGB}{2, 68, 247}

\newcommand{\comm}[1]{\textcolor{myblue}{#1}}

\sloppy


\begin{document}

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\huge\textsc{TDK}
\HRule \\[0.4cm]
{ \huge \bfseries Chaos in Large Language Models}\\[0.2cm] 
\HRule \\[0.5cm]
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
Author: \\
\Large\textbf{Jaca Gregorio}\\
\large{Physicist Engineering BSc, III. year.}
\end{flushleft}
\end{minipage}
\qquad
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft}\large
Supervisors: \\
\Large\textbf{Dr. Török János}\\
\large Associate professor \\
BME Elméleti Fizika Tanszék

\Large\textbf{Kristóf Benedek}\\
\large PhD Candidate in Physics \\
BME Elméleti Fizika Tanszék

\end{flushleft}
\end{minipage}
\\[6cm]
\includegraphics[scale=0.7]{plots/bme.png}\\[0.2cm]
\large{\textbf{BME}}\\
\large{\textbf{2025}}
\vfill
\end{titlepage}

% \newpage\null\thispagestyle{empty}\newpage

%%% --------- GUIDELINES ---------- %%%
% This doc is in progress. I write but leave many comments to remind myself where to go back and improve. Wherever you see a "(GGG)" it means that I'm not happy with the wording and it needs to be made more precise and clear.
% When I use "G:" i am straightforwardly expressing an opinion 
% It's easier for me if you use your initials before your comments, so i can quickly understand who they come from

% Things I'm working on:
% Intro is very bad
% Results needs more quantitative data: RQA table and maybe some lyapunov fit if possible
% I have too many sections and things. I will at the end remove the extras. If you think anything in the main text should not be there, leave a comment.
% I'm still developing this idea that "stretching and folding" which is intuitively what happens in chaotic systems, and which parts of the LLM architecture cause it
% explain better why and how sliding window are used for the distance metrics. how this is analogous to the sentence embeddings (where a whole sentence is converted into a vector). and how this helps account for the context which makes sense to do because LLMs are more like a delay system, where the state is not defined by only a single hidden state, but also the ones in its context window. I will just mention this, which I think is reasonable.
% there are some metrics missing in methods. I'm still not 100% which I'll include and which not. the ones currently in methods I will probably include (except "PCD", which maybe won't)
% I will adress all 'GGG' marked issues

% Sections which are mostly finished:
% Related Work - Preliminaries - Methods


% GENERAL TODO:
% add labels to each section, subsection, plot
% introduce abbreviations only once. more consistency in notation


\begin{abstract}
    Large Language Models (LLMs) have achieved remarkable performance across a wide range of tasks, yet their internal dynamics remain poorly understood. In this work, we apply the tools of nonlinear dynamics and chaos theory to LLMs. By analyzing both text and hidden state trajectories, we demonstrate that LLMs exhibit hallmark signatures of chaos, including sensitivity to initial conditions and a positive maximum Lyapunov exponent, with consistent results across different distance metrics. Recurrence plots show structural similarities between LLMs and canonical chaotic systems such as the Lorenz attractor, while dimension analysis reveals fractal structures in the hidden state space, particularly pronounced in the last layers. We propose that the nonlinear coupling induced by attention mechanisms plays a key role in driving this chaotic behavior. 
\end{abstract}
\newpage
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\newpage

\section{Introduction and Motivation}
\label{sec:introduction}
Large Language Models (LLMs) are among the most complex artificial systems currently studied. As deep neural networks composed of billions of parameters, they are highly nonlinear and high-dimensional. Unlike classical dynamical systems, where governing equations are explicitly defined, the parameters of LLMs are learned through during training on massive text corpora. This allows these models to learn huge amounts of knowledge with little human intervention, which allows for scaling their size, resulting powerful models with intricate dynamics whose behavior is often unpredictable and difficult to interpret.
% provide a fascinating case study for nonlinear dynamics

Despite this opacity, LLMs exhibit stable generative behavior: they maintain coherence over long text sequences while allowing for variability in their responses. This tension between stability and sensitivity is reminiscent of chaotic systems in nonlinear dynamics, systems that remain bounded yet display sensitive dependence on initial conditions. Understanding whether the internal evolution of LLM representations exhibits similar characteristics offers a new perspective on both interpretability and model behavior.
% provide a fascinating case study for nonlinear dynamics
% we could frame the LLM as a non-autonomous dynamical system, where the input sequence (prompt) acts as a time-dependent driving force. This distinguishes it from classical autonomous systems like the Lorenz attractor and might have implications for the stability and types of attractors we observe. ??
% A common theme with LLMs is that there is very little that we explicitly instruct them to do, rather we create the architecture and they learn everything during trainig

There is growing interest in the sensitivity of LLMs (and other generative deep neural networks) to initial conditions and small perturbations. For various applications, like Reinforcement Learning (RL), determinism and reproducibility are desirable in order to keep the RL on-policy, while for other uses like synthetic data generation, paraphrasing, and novel image generation, diversity is desired. Understanding whether this sensitivity reflects stochastic noise or deterministic chaos is essential for understanding how reasoning and stability coexist in these models.

% Understanding fixed/stable points in LLMs can clarify their generative capability constraints.
% G: needs a more natural phrasing, not forcing it into the SIC chaos direction until the end
% Does the model operate in a regime that is chaotic enough to allow for creativity, but stable enough to maintain coherence? 

% G: also these tools can be applied to other DNNs that handle audio image video
% this dynamical systems perspective is a general framework applicable to any generative model that operates sequentially, not just text-based LLMs.

Much research interest has been placed on interpretability, understanding how LLMs learn, store facts, high level concepts and how they reason. Because of their large size, complexity, and number of parameters, LLM dynamics can not be studied analytically.
Interpretability research has taken a bottom up approach, looking for mechanistic explanations of model behavior, from identifying circuits (small, interpretable subnetworks, toy models, which can be fully understood) to analyzing attention patterns \cite{olah2020zoom, ameisen2025circuit, lindsey2025biology}. Chaos-theoretic tools complement this line of work by quantifying emergent global properties such as stability, divergence, and attractor structure, rather than focusing solely on small scale mechanisms. Interpretability aims to understand how these networks do what they do, and chaos analysis may help reveal how they evolve dynamically across internal representations.

Working with LLMs presents several challenges, such as the discrete nature of tokens, their hidden state representations, and the high dimensionality of the embedding space. Latter being a particularly hard problem since we are talking of dimensionality of order $10^3$. Having a large dimension as this implies that classical methods fail out of the box.
The state of the system includes not only the last token, but also the context window, and measuring distance between states is non-trivial.
Extending these tools to high-dimensional neural representations requires reformulating what constitutes a trajectory, a state, and a meaningful distance in the embedding space. 
%In particular, if LLMs perform reasoning in a structured, low-dimensional manifold of this space, one might expect ordered, non-chaotic evolution; yet if reasoning involves self-reinforcing feedback and contextual amplification, chaotic signatures may emerge. 
We have worked around many of these challenges, and we hope this experience can be transferred to the study of other high dimensional systems.

In this paper, we apply tools from nonlinear dynamics to the study of LLM dynamics evolution during inference. We define trajectories in hidden-state and text space, explore metrics to quantify divergence between nearby trajectories, and apply recurrence and dimensionality analyses to detect possible chaotic structures. The framework presented here extends beyond text models and can, in principle, be applied to any high-dimensional autoregressive generative system, such as audio, image, or multimodal models.

% Another challenge is the non-stationarity. The 'rules' of the system (the attention patterns) are state-dependent, changing at every single step. This is unlike classic chaotic systems where the governing equations are fixed. This makes analysis much harder but also much more interesting.

\section{Related Work} 
\label{sec:related_work}
% This section quite common in ML papers. 
% This section briefly summarizes the main results from other papers which inspired my analysis and also explain some of the results obtained. It also should reinforce the idea that it's justifiable to treat LLMs and chaotic nonlinear systems. 

The application of dynamical systems theory to neural networks has revealed complex behaviors such as attractors, bifurcations, and chaotic transients in both recurrent and feed-forward architectures. Recent work has extended these ideas to Transformer models, motivating the present study.
% We see this work as an expansion of this, focusing on LLMs and bringing the tools from chaos theory.

\paragraph{Chaotic Dynamics in LLMs:}

\cite{li2025cognitive_activation} proposes that LLMs' reasoning capabilities are, in fact, chaotic processes of dynamic information extraction in parameter space.  Introduces the Quasi-Lyapunov Exponents to quantify chaotic characteristics across model layers and shows sensitivity to initial conditions.
\cite{geshkovski2025mathematicalperspectivetransformers} model self-attention as a nonlinear coupling between tokens.
\cite{dynamicalmeanfieldtheoryselfattention} demonstrates that even simplified self-attention transformer networks with 1-bit tokens and weights exhibit nontrivial dynamical phenomena, including non-equilibrium phase transitions and chaotic bifurcations. 
\cite{tomihari2025recurrent_self_attention_dynamics} shows that normalization layers also normalize the Jacobian's complex eigenvalues, bringing the dynamics close to a critical state with maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos: a critical regime where signals neither explode nor vanish, enabling long-range information propagation.
% G: this can explain if the trajectories have sub-exponential divergence
%show that normalization layers effectively suppress the Jacobian's spectral norm and control oscillatory behaviors. Their empirical findings reveal that high-performance self-attention models exhibit maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos—a critical regime where signals neither explode nor vanish, enabling long-range information propagation.
% this provides a mechanistic explanation for why the system might hover at the edge of chaos. The normalization layers act as a dissipative force, counteracting the explosive stretching from other parts of the network, keeping the system bounded and stable. This connects directly to the idea of 'folding'.

\paragraph{Determinism and Attractors in Language Models:}
% The tension between reproducibility and novely in LLMs has become increasingly relevant. 
Recent engineering efforts \cite{he2025nondeterminism} have focused on achieving deterministic inference by controlling GPU batching variance and floating-point errors, motivated by the need for reproducibility and clear signals in reinforcement learning training. Understanding LLM's sensitivity to batch combinations, perturbations and input data is useful for this branch of study.
\cite{wang2025unveiling_attractor_cycles} shows that LLMs used for paraphrasing converge to periodic attractor cycles, reducing linguistic diversity.
\cite{cyclegan} % Comment: PLOS Complex Systems article - 
demonstrates that CycleGAN image generators' space is more limited than the training data's, having positive Lyapunov exponents and attractor dimensions similar to the training data intrinsic dimension. Chaotic dynamics contribute to the diversity of the generated images.
%This suggests that generative models may naturally converge to stable configurations that reduce output diversity. 
% G: this is a bit contradictory bc I say that model generation: more limited than training data but chaotic makes more diverse.
% This isn't necessarily a contradiction. The system can be chaotic (positive Lyapunov exponent, sensitive dependence) which allows for diversity, but still be confined to an attractor whose dimension is smaller than the ambient space. This is the definition of a *strange attractor*. The chaos creates novelty *within* the bounds of the learned manifold.

\paragraph{Interpretability Through Dynamical Analysis:}
\label{par:interpretability_dynamics}
%The dynamical systems perspective has proven valuable for understanding recurrent architectures. 
\cite{sussillo2013} showed that fixed points and linearized dynamics in trained RNNs provide interpretable insights into network function. \cite{zhang2024intelligence_edge_of_chaos} found that intelligence emerges at an optimal complexity level, a regime neither fully ordered nor chaotic or random. Systems that are either highly chaotic or perfectly periodic exhibit poor downstream performance, suggesting a "sweet spot" conducive to intelligent behavior at the edge of chaos. \cite{zhou2025geometryreasoningflowinglogics} shows how LLM's chain-of-thought reasoning can be interpreted as smooth flows in embedding space, where logical statements control the flow's velocities.
% G: grego this section and see where in the text you should cite this papers etc...

\section{Preliminaries}
\label{sec:preliminaries}

% This section describes the background knowledge required to understand Nonlinear Dynamics and Chaos, LLMs, and their similarities and differences. This provides us with the starting point for this paper.

\subsection{Nonlinear Dynamics and Chaos}
\label{subsec:nonlinear_dynamics}

% <start improve>
A \textbf{dynamical system} characterizes how the state of a system evolves over time according to deterministic rules. The state of the system is represented by a point $\mathbf{x}(t)$ in a \textbf{phase space}, whose coordinates correspond to relevant system variables. In continuous time, evolution is typically governed by a set of ordinary differential equations (ODEs):
\begin{equation}
    \frac{d\mathbf{x}}{dt} = \mathbf{F}(\mathbf{x}),
\end{equation}
while in discrete time, it is described by maps:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{f}(\mathbf{x}_t).
\end{equation}
A system is said to be \textbf{deterministic} if its future state is completely determined by its present state. The trajectory $\mathbf{x}(t)$ traces the temporal evolution of the system in phase space. The geometry of this trajectory encodes the qualitative behavior of the system, whether it settles to equilibrium, oscillates periodically, or exhibits chaotic motion. 
% <end improve>

% Linear systems, while analytically tractable, cannot produce the rich, aperiodic behaviors associated with chaos. Nonlinearity, arising when $\mathbf{F}$ or $\mathbf{f}$ involve products, powers, or nonlinear couplings of state variables, enables multiple time scales, feedback loops, and instabilities. This results in phenomena such as bifurcations, quasi-periodicity, and ultimately, chaos. For instance, as a control parameter $\mu$ varies, a nonlinear system may undergo a sequence of \textbf{period-doubling bifurcations}, leading to an accumulation point $\mu_\infty$, beyond which the motion becomes chaotic (as shown famously in the logistic map).
% G include or not?

Chaotic systems are a subclass of deterministic nonlinear systems that exhibit complex and practically unpredictable behavior. This apparent randomness arises not from a stochastic nature but from the system's intrinsic dynamics. The defining characteristics of chaos are non-periodicity and sensitivity to initial conditions (SIC). This property implies that trajectories originating from infinitesimally close initial states, say $\mathbf{x}(0)$ and $\mathbf{x}(0) + \bm{\delta}(0)$, will diverge exponentially over time. The separation distance $\|\bm{\delta}(t)\|$ grows according to:
\begin{equation}
    \|\bm{\delta}(t)\| \approx \|\bm{\delta}(0)\| e^{\lambda t}
\end{equation}
where $\lambda$ is the \textbf{maximal Lyapunov exponent}. A positive maximal Lyapunov exponent is a strong indicator of chaos. It signifies that any measurement error or uncertainty in the initial state, no matter how small, will be amplified exponentially, making long-term prediction impossible. 
In general, a dynamical system evolving in an $n$-dimensional phase space possesses a spectrum of $n$ Lyapunov exponents, $\{\lambda_1, \lambda_2, \ldots, \lambda_n\}$. Each exponent quantifies the rate of exponential divergence or convergence along a different direction in phase space. The \textbf{maximal Lyapunov exponent} determines the overall chaotic nature of the system.
%: if $\lambda_1 > 0$, trajectories diverge in at least one direction, indicating chaos. The remaining exponents characterize contraction (if $\lambda_i < 0$) or neutral behavior (if $\lambda_i = 0$) along other directions. This spectrum reveals that chaotic systems simultaneously exhibit both expansion and contraction in different directions, leading to the formation of strange attractors with fractal structure.
% Differently put, systems with $\lambda < 0$ converge to stable equilibria, $\lambda = 0$ corresponds to periodic or quasiperiodic motion, and $\lambda > 0$ denotes chaos. This exponential separation is the mathematical origin of the “butterfly effect”.

Despite their practical unpredictability, often the trajectories of dissipative chaotic systems are not random; frequently they are confined to a bounded, lower dimensional subset of phase space, a \textbf{strange attractor}, towards which all close trajectories get asymptotically attracted but without having a periodic motion \footnote{In general, if a map contracts volumes in phase space it is called dissipative. A physical system with friction is an example. In contrast, conservative systems cannot have strange attractors because attractors attract all trajectories in a small open set containing it, which contracts the phase space volume.}.
\footnote{The 'strangeness' partly comes from the fact that trajectories diverge exponentially but remain confined in a zero-volume subset of the phase space. The explanation for this is that they diverge in the direction parallel to the attractor, with positive lyapunov exponent, and get attracted in the direction orthogonal to it, with negaitve lyapunov exponent.}.
Intuitively, such systems combine stretching (which causes divergence) with folding (which confines trajectories), resulting in intricate, fractal-like structures of non-integer dimension. 

The \textbf{correlation dimension ($D_2$)} is a characteristic measure of strange attractors \cite{GRASSBERGER1983189}. This technique can be used to distinguish between deterministic chaos and random noise, and it is closely related, but not equal to the fractal and information dimension \cite{GRASSBERGER1983189}. For purely periodic systems, $D_2 = 1$, while for chaotic systems it takes non-integer values, indicating a fractal attractor structure (see Sec. \ref{subsec:dimensionality_methods}).

Another powerful tool for analyzing dynamical systems is \textbf{Recurrence Analysis} \cite{MARWAN2007237}. A Recurrence Plot (RP) visualizes the times at which a trajectory revisits a previously visited neighborhood in phase space (see Sec. \ref{subsec:recurrence_analysis_methods}). %, within a threshold $\epsilon$. The recurrence matrix is defined as:
% \begin{equation}
%     % R_{i,j}(\epsilon) = \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|), \quad i,j = 1, \dots, N
%     R_{i,j}(\epsilon) = \Theta(\epsilon - \text{Distance}(\mathbf{x}_i, \mathbf{x}_j)), \quad i,j = 1, \dots, N 
% \end{equation}
The visual patterns in a RP provide an insight into the type of dynamics of the system: periodic systems produce long, parallel diagonal lines, the distance between them corresponding to the periods of oscillations, random systems show a uniform, noisy plot, and chaotic systems display a complex geometric structure with short, interrupted diagonal lines. \textbf{Recurrence Quantification Analysis (RQA)} quantifies these structures by extracting metrics such as recurrence rate, determinism, laminarity, and entropy to characterize dynamical complexity.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/rp_periodic_lorenz_stoch.png}
    \caption{Exemplary recurrence plots of (A) a periodic motion with one frequency, (B) the chaotic Lorenz system, and (C) of normally distributed white noise. Figure taken from \cite{DONNER_2011}}
    \label{fig:rp_examples}
\end{figure}

% G: I would like to generally describe the necessary conditions for chaos. (I dont mean SIC and exponential divergence, but rather the underlying characteristics of the system which will make it be chaotic). Then I'd like to explain which components of LLM architecture correspond to these. example: nonlinear coupling and attention

Chaos generally arises when a system satisfies the following structural mechanisms \cite{Hnon1976ATM, ROSSLER1976, strogatz_textbook}:
\begin{itemize}
    \item \textbf{Nonlinearity:} enables feedback amplification and coupling between degrees of freedom.
    \item \textbf{Stretching:} separates nearby trajectories, quantified by positive Lyapunov exponents.
    \item \textbf{Folding:} confines trajectories to a bounded region, preventing divergence to infinity.
\end{itemize}
% GGG unify this stretching and folding discussion

% GGG
% The interplay of stretching and folding leads to a balance between divergence and confinement, the geometric origin of strange attractors. Classical examples embodying these principles include the Hénon map \cite{Hnon1976ATM}, the Rössler system \cite{ROSSLER1976}, and the Lorenz equations \cite{strogatz_textbook}. Together, they form the canonical models for studying chaos, predictability, and low-dimensional turbulence.

% maybe "In LLMs, we hypothesize that the self-attention mechanism provides the 'stretching' by amplifying small differences in token representations, while normalization layers and the bounded nature of activation functions provide the 'folding', ensuring the system's states remain within a finite volume."



\subsection{Large Language Models}
\label{subsec:llms}
Large Language Models are a class of deep neural networks, typically using the Transformer architecture. They are trained on massive text data to predict the probability distribution for the next token in a sequence, and used autoregressively to generate a text output sequentially by sampling from that distribution.

The process begins with tokenization, where input text is broken down into a sequence of integers (tokens) \footnote{The tokens represent fragments of text e.g: "hello", "\textbackslash n", "**", "1", "token", "ization", " ". Tokenization is learned by the model during training.} Each token is then mapped to a high-dimensional vector, its embedding.
\begin{equation}
    \mathbf{e}_i = E[t_i], \quad i = 1, \dots, L,
\end{equation}
where $E \in \mathbb{R}^{V \times d}$ is the embedding matrix, $V$ the vocabulary size, $d$ the embedding dimension, and $L$ the sequence length. Embeddings serve as initial representations of tokens in a high-dimensional semantic space. 
%During training, both the embedding matrix and the tokenization scheme (e.g., byte-pair encoding) are optimized to capture subword-level regularities in language. 

The embeddings are processed through a stack of Transformer layers. Hidden state refers to the vectors that go through the model's layers \footnote{Embeddings are static, initial vector representations of tokens, while hidden states are dynamic, context-dependent vectors that change as the model processes information through its layers. Embeddings provide an initial meaning for a token, whereas hidden states carry the evolving context, including information from previous tokens in the sequence.}. Each layer is primarily composed of two sub-modules: a multi-head self-attention mechanism and a position-wise feed-forward network called Multi-Layer Perceptron (MLP). The self-attention mechanism is the key source of nonlinearity and contextual understanding, allowing the model to dynamically weigh the importance of all other tokens in the context window, and then update the representation of a single token. Each attention head can be expressed as \cite{attention}:
% G: somewhere I could be more precise on what first layer and last layer refers to and entails, so that later in results it is more clear. maybe in method explain that i extract the hidden states after each transformers 

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where $Q$ (Query), $K$ (Key), and $V$ (Value) are learned low-rank matrices. The Query represents the current token's focus, the Key represents the information other tokens have, and the Value contains the content of those tokens, with each attention layer looking at different information. The attention mechanism computes a weighted sum of the Values, where the weights are determined by the similarity between the Query and Keys \footnote{As an intuition, attention is like a learned differentiable lookup table. Given a hidden state, it generates a query and returns a weighted sum of values based on the key-query similarity, which is used to update the hidden state. The queries, keys and values are generated from the hidden states in the context window using Q, K and V.}. At each layer there are multiple attention heads which operate in parallel on the same input hidden state. Their output is added to the hidden state, which then gets normalized and goes to a MLP module.
% G: each heads output is of dimension d_model / n_heads , and then the output from the heads is concatenated, bringing back a d_model dimensional vector which is added to the hidden state
In LLMs causal masking is used to ensure that a token is only influenced by previous (and not future) tokens during attention 
\footnote{Nowadays most LLMs are decoder-only and it is necessary to use causal masking for autoregressive generation. Bidirectional (no causal masking) attention can be used in the encoder of a transformer-based model, such as BERT (encoder only) or some translators (encoder-decoder).}.
%\footnote{This is necessary in order to use LLMs autoregressively. Other transformer-based models like}.
Due to the compute cost of attention, which grows with the square of the sequence length, attention is only performed on a sliding context window \footnote{Attention is performed on the minimum value between the sequence length and the context window size.} with the latest token at its right endpoint.
This operation creates a nonlinear feedback and coupling mechanism between all elements of the sequence. The MLP further processes these representations through additional nonlinear transformations. Normalization layers like LayerNorm and RMSNorm normalize the hidden states and project onto a lower-dimensional ellipsoid.
%This is a bounding operation, preventing state vectors from growing infinitely and thus providing the "folding" part of the "stretching and folding" mechanism required for chaos. 
% The output of the final layer is a sequence of hidden states, which represent the contextualized meaning of each token. % G: clarify that for all layers we can get it and also connect it better to how the logits are obtained

During inference, the LLM generates tokens autoregressively, which can be viewed as a discrete-time dynamical system:
\begin{equation}
    % \mathbf{s}_{t+1} = f(\mathbf{s}_t, \mathbf{s}_{t-1}, ..., \mathbf{s}_0) % G: maybe it should be \mathbf{s}_{t-context_window_size} to be more precise
     s_{t+1} = f(s_t, s_{t-1}, ..., s_{t-context size+1})
\end{equation}
where $\mathbf{s}_t$ represents the hidden state at step $t$. 

To generate the next token, the hidden state of the last token is passed through a final linear layer, the unembedding, to produce logits over the entire vocabulary, which are then converted to a probability distribution via a softmax function. A decoding strategy is then used to select a token. \textbf{Greedy sampling} deterministically selects the token with the highest probability. In contrast, \textbf{probabilistic sampling} introduces stochasticity, often controlled by a temperature parameter ($T$). Higher temperatures flatten the distribution, making the output more diverse.



\subsection{Analogy and Limitations}
\label{subsec:analogy_limitations}
We propose viewing the autoregressive operation of a Large Language Model as a \textbf{high-dimensional, discrete-time dynamical system}. Each forward pass defines a nonlinear state-dependent mapping between successive states of the system, where the collection of hidden states in the context window constitutes the trajectory evolving within the model's embedding space. From this perspective, the \textbf{self-attention mechanism} serves as a state-dependent nonlinear coupling between different components of this trajectory, analogous to the inter-variable coupling observed in classical chaotic systems. This acts as the stretching component: small perturbations to one token's hidden state can cause large changes in attention weights, which in turn significantly alter other token representations. The \textbf{normalization layers} (e.g., LayerNorm, RMSNorm) and bounded activation functions act as folding mechanisms, constraining the trajectory within a finite subspace by projecting hidden states onto normalized manifolds. This is explained in more detail in the Appendix \ref{subsec:layer_norm}. The alternation of stretching and folding operations across layers yields an evolution that is both bounded and sensitive to initial conditions.

% Each token's representation is affected by (previous) other tokens, with softmax, ReLU, layer norm providing the nonlinearity.

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Chaotic System} & \textbf{LLMs} \\ \midrule
Phase Space & Embedding / Hidden State Space \\
State Vector & Hidden State Vector \\
Phase-Space Trajectory & Sequence of Token Text or Hidden States \\
Continuous Time Evolution & Discrete Token Generation Step \\
Differential Equation & Autoregressive Function \\
Numerical Integration & Forward-Pass Inference \\
Nonlinear Coupling & Self-Attention Mechanism \\
Stretching & Attention / MLP Layers \\
Folding & Normalization Layers \\
Initial Condition & Initial Prompt Text or Embedding \\
Measurement Uncertainty & Floating-Point Precision, Quantization \\ \bottomrule
\end{tabular}
\caption{Analogies between concepts in chaos theory and LLMs.}
\label{tab:analogy}
\end{table}
% G: could add stretching and folding. table too big rn
% could add "Stretching" -> "Attention/MLP layers" and "Folding" -> "Normalization layers". 

Furthermore, the multi-head attention architecture can be interpreted as an ensemble of coupled dynamical subsystems, each operating in a distinct subspace. Their outputs are combined through learned linear projections, introducing another layer of nonlinear interaction analogous to multi-variable coupling in chaotic flows such as the Lorenz or Rössler systems. The overall model thus represents a structured, multi-dimensional extension of the canonical "stretching–folding" paradigm observed in dissipative chaotic systems.

However, the analogy is not perfect and has critical limitations.
\begin{itemize}
    \item \textbf{Discrete-Time and Sampling Effects.} LLM dynamics are discrete, not continuous, with a fixed "time step" of one token. The act of sampling a token from the probability distribution to generate the next token can be seen as a "collapse" or a sharp discontinuity in the trajectory's evolution \footnote{The hidden states at the first layer are also discrete and correspond exactly to a column of the embedding matrix. In deeper layers, the possible hidden states are more numerous (and effectively continuous), as they are generated through nonlinear transformations (transformer layers) of the previous hidden representations.}.
    % G: can clarify that it matters in that this new token is the starting point for the processing in the 1st layer, but all the previous hidden states remain and will influence at each layer.
    \item \textbf{Stochasticity and Non-Determinism.} LLMs are usually used with probabilistic sampling, which adds a random component and makes it non-deterministic. Furthermore, the dimensionality of the state space is extraordinarily high (thousands of dimensions), which poses significant challenges for traditional chaos analysis techniques.
    \item \textbf{Extreme Dimensionality.}  
    The dimensionality of modern LLM state spaces (typically thousands of hidden dimensions across hundreds of context positions) vastly exceeds that of classical chaotic systems. This high dimensionality leads to strong concentration of measure effects and challenges the direct application of traditional chaos analysis tools, which are often developed for low-dimensional, smooth manifolds.

    \item \textbf{Nature of Coupling.}  
    In classical systems (e.g., Lorenz equations), coupling occurs between fixed state variables (e.g., $x$, $y$, $z$). In Transformers, coupling occurs across token positions, that is, between distinct instances of the system at different effective "times" within the same context. This results in a higher-order coupling structure where each state variable is itself a vector embedding, leading to emergent behaviors that differ qualitatively from classical variable coupling.

    \item \textbf{Nonstationarity and Weight Freezing.}  
    The weights of a trained LLM remain static during inference; all apparent "dynamics" arise solely from transformations of the input representations. This differs from physical dynamical systems, whose governing equations continuously evolve in real time. Thus, while the forward pass can be viewed as a dynamical process, it does not constitute an autonomous system in the classical sense.

\end{itemize}


% G: also in chaotic systems the nonlinear coupling is between variables, but in llm it is between tokens, which would be the state of the system (so all the variables) at different times
% ?? In a classical system like Lorenz, the coupling is between fixed variables (x, y, z). In a Transformer, the coupling is between *positions* in a sequence, and the 'variables' at those positions are entire state vectors. It's a much more complex, higher-order form of coupling. Worth stating this clearly. ??? GGG

% G: as i understand it. MLP provides nonlinearity, attention nonlinear coupling and (maybe) stretching. normalization keeps the system bounded (folding). layer norm is more like projecting to the plane perpendicular to (1,1,1,..,1) plus affine rescale.

% G: is normalization folding or stretching? normalize would siggest constraining so folding, but attention is cross communication so also folding. talk this out and dicuss. which processes in LLM constrain and maintain boudedness? is this folding? which stretch= is stretching individual?
%  Normalization is probably folding/constraining. Attention is primarily stretching. attention allows one state vector to be influenced by many others. If a small change in one vector causes it to attend more strongly to a very different vector, its representation can be 'stretched' dramatically towards the VALUE (from K-Q) other vector. The MLP layers, with their non-linear activations, also contribute to stretching. The folding comes from normalization layers and activation functions like tanh that are inherently bounded.

\section{Methods}
\label{sec:methods}

This section describes the methodological framework used to investigate chaoticity in Large Language Models (LLMs). We quantify divergence and structure in the model's hidden-state trajectories using a suite of distance metrics, recurrence analysis, and dimensionality estimation techniques derived from nonlinear dynamics. Finally, we detail the experimental setup and the procedure used to generate perturbed initial conditions.

\subsection{Trajectory Distance Metrics}
\label{subsec:trajectory_distance_metrics}
We use a suite of metrics to quantify the divergence between pairs of trajectories, $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n)$ and $\mathbf{Y} = (\mathbf{y}_1, ..., \mathbf{y}_m)$. We employ a set of complementary metrics capturing both local and global differences. Distances between vector representations of text have previously been shown to encode semantic relationships and structural similarity \cite{text2vec, reimers2019sentencebertsentenceembeddingsusing}. Since there is no canonical distance measure in high-dimensional semantic spaces, we adopt multiple complementary metrics to ensure robustness and interpretability. 


% G: also mention that some metrics are based on other metrics: like rank eigen uses cos similarity to compare pairs of vectors. check which use L2
% We justify the use of cosine similarity as it is a meaningful metric both in the hidden state and embedded text space.
% "Since there is no single canonical distance metric for high-dimensional semantic spaces, we employ a diverse suite of metrics. This ensures our findings are robust and not an artifact of a particular choice of measurement. Each metric captures a different aspect of trajectory divergence, from pointwise orientation (Cosine) to global shape (Fréchet, Hausdorff)."

\subsubsection{Vector-wise Metrics}
\label{sssec:methods_vector_metrics}
These metrics are applied pointwise between corresponding vectors of two trajectories, yielding a time series of distance values.
\begin{itemize}
    \item \textbf{Cosine Distance:} Measures the angle between two vectors, capturing their orientation similarity. It is widely used to compare similarity between vector embeddings \footnote{If all vectors are normalized, cosine distance and Euclidean distance are monotonically related, so they produce the same relative ordering of pairwise similarities even though their numerical values differ. Empirically the results obtained are very similar.}. It is defined as $1 - \text{Cosine Similarity}$:
    \begin{equation}
        d_{cos}(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{y}\| \|\mathbf{x}\|}
    \end{equation}
    % \item \textbf{Normalized Cross-Correlation:} Measures the similarity between two vectors as a function of the displacement of one relative to the other. For discrete vectors at zero lag, it is related to the cosine similarity. 
\end{itemize}

\subsubsection{Trajectory-wise Metrics}
\label{sssec:methods_trajectory_metrics}
Trajectory-wise metrics compare entire trajectories as geometric objects rather than as aligned sequences of vectors. These metrics yield a single scalar value for the distance between two entire trajectories. To obtain a divergence curve, we apply these metrics in a sliding window of increasing size. For all metrics, Euclidean distance is used as the base distance measure between individual points.

\begin{itemize}
    \item \textbf{Dynamic Time Warping (DTW) \cite{SalvadorChan2007}:} Finds an optimal non-linear alignment between two sequences. It computes a cost matrix where $C_{i,j}$ is the distance between $\mathbf{x}_i$ and $\mathbf{y}_j$. The DTW distance is the value in the final cell of an accumulated cost matrix $D$, defined by the recurrence:
    \begin{equation}
        D_{i,j} = C_{i,j} + \min(D_{i-1, j}, D_{i, j-1}, D_{i-1, j-1})
    \end{equation}
    \item \textbf{Fréchet Distance \cite{Denaxas2023} \cite{EiterMannila1994}:} Intuitively, the minimum length of a leash needed to connect a person and a dog walking along their respective paths. It is sensitive to the ordering of points. For two trajectories $X$ and $Y$, it is:
        \begin{equation}
            d_F(P, Q) = \min_{\text{couplings}} \max_{(i, j) \in \text{path}} \| X_i - Y_j \|
        \end{equation}
        In this work, we use the discrete Fréchet distance, using monotonic alignments of the sampled points, respecting their order but without continuous reparameterization.
        % G: it is not very clear. 
        %G: using monotonic alignments -> which finds the minimal leash length over all monotonic alignments of the sampled points (discrete Fréchet).
    \item \textbf{Hausdorff Distance \cite{SciPyDirectedHausdorff}:} Measures the greatest of all distances from a point in one set to the closest point in the other set. It is a pure set-based distance.
    \begin{equation}
        d_H(X, Y) = \max \left( \sup_{\mathbf{x} \in X} \inf_{\mathbf{y} \in Y} \|\mathbf{x}-\mathbf{y}\|, \sup_{\mathbf{y} \in Y} \inf_{\mathbf{x} \in X} \|\mathbf{x}-\mathbf{y}\| \right)
    \end{equation}
    % implementation computes the symmetric Hausdorff as max(directed_hausdorff(a,b), directed_hausdorff(b,a)), that is standard.
    % For sliding-window analysis, hausdorff.py does not compute a strict per-window Hausdorff in all cases but computes per-point nearest distances and then aggregates those distances by either "max_of_mean" (default) or "mean_of_max" (configurable). This is a deliberate pragmatic choice;  it changes sensitivity (averaging vs extreme) and makes the sliding-window Hausdorff different from the textbook per-window Hausdorff if you choose mean-based aggregation.
    \item \textbf{Principal Component Dissimilarity (PCD):} We also defined a new distance metric based on the structural similarity of two trajectories. This metric compares the geometric structure of two trajectories by analyzing the alignment of their principal components (PCs). For each PC in the first trajectory, it finds the most similar PC in the second. The dissimilarity is then calculated as the total cosine distance between these matched pairs of principal components. This is explained in detail in \ref{subsec:pca_distance}.

    \item \textbf{Cross-Correlation Dissimilarity \cite{pearson} \cite{scipy_pearson}:} This metric quantifies the structural dissimilarity between two trajectories by comparing their internal self-similarity structures. First, a cosine self-similarity matrix ($S_X$ and $S_Y$) is computed for each trajectory and the Pearson correlation ($\rho$) between both. The dissimilarity is defined as one minus the correlation, computed as:
    \begin{equation}
        d = 1 - \rho = 1 - \frac{\operatorname{cov}(S_X^\text{flat}, S_Y^\text{flat})}{\sigma_{S_X^\text{flat}} \sigma_{S_Y^\text{flat}}}
    \end{equation}
    where $S^\text{flat}$ represents the flattened matrix, $\operatorname{cov}$ is the covariance, and $\sigma$ is the standard deviation.

    % G: wasserstein not included. but maybe i wont use it
\end{itemize}

\subsubsection{Sliding Window Analysis}
\label{sssec:methods_sliding_window}
To generate a time-series distance from the trajectory-wise metrics, which naturally yield a single scalar, we use a sliding window that moves along both trajectories simultaneously, computing a single distance value for each window. This process results in a time series of distances. The use of a sliding window is motivated by the fact that an LLM's state depends on a history of previous states (the context window).

The standard Cosine Distance metric can optionally be used with a sliding window. For each window, it computes the mean vector of all vectors within that window. This creates new, shorter trajectories where each point represents the average of a window from the original trajectory. The final time series is then computed by performing the standard one-to-one cosine distance calculation.



% The comparison between the two windowed segments is performed using one of two general strategies.
% For most metrics (including DTW, Fréchet, Cross-Correlation Dissimilarity, and Principal Component Dissimilarity) the two trajectory segments are treated as complete entities, and the metric function is applied to the segments as a whole to produce a single scalar distance for that window.
% The Hausdorff distance uses a different approach, as it is a metric between sets of points. Within each window, it first computes the distance from every point in one segment to its single closest point in the other. This is performed for both segments. These lists of point-wise minimum distances are then aggregated into a single scalar for the window. The aggregation finds the maximum value from each list (the two directed Hausdorff distances) and then takes the mean of these two maximums.

% \item \textbf{Focal-Point Comparison:} A special case exists for applying windows to element-wise metrics like Cosine Similarity. While not strictly necessary, a windowed version is implemented in the `cos_sim` metric. This method takes the single vector at the center of one windowed segment and compares it against all vectors in the other segment, yielding a list of cosine similarities. The mean of this list is calculated. This process is done symmetrically for both directions, and the two resulting mean-similarity values are themselves aggregated (e.g., via another mean) to produce the final scalar similarity for the window.
% The standard Cosine Distance (`cos`) metric, which is fundamentally a one-to-one vector comparison, can also be used with a sliding window, but in a unique way. If a window size is specified, the metric first transforms the trajectories. For each window, it computes the \textit{mean vector} of all vectors within that window. This creates new, shorter trajectories where each point represents the average of a window from the original trajectory. The final time series is then computed by performing the standard one-to-one cosine distance calculation on these new, pre-aggregated trajectories.

\subsection{Measuring Lyapunov Exponents}
\label{subsec:methods_lyapu}

There were some challenges inherent to the system under study. In any bounded system (as vectors are normalized), there is a maximum possible value for the distance, and the distance between two diverging trajectories will eventually reach the maximum and saturate. Therefore, in order to measure the lyapunov exponents we excluded this saturated region.

Using a sliding window of size 16 \footnote{using sliding window size of 8 or 32 yielded similar results}, we computed the Cosine Distance between 100 perturbed trajectories from the same prompt (see Section \ref{subsec:init_cond_gen}) using the hidden states from the last layer. We also did it using a sliding window of 8. We fit an exponential on each starting from the first token and finding the best token to end the fit by optimizing the coefficient of determination $r^2$. Finally, because some trajectories never diverge, we excluded these by only considering those with positive lyapunov exponents where the $r^2$ was greater than 0.85. Because the distance metrics' base noise is comparable to the perturbation magnitude, we could not accurately measure negative lyapunov exponents, and we limited ourselves to reporting positive ones from the spectrum. 

\subsection{Recurrence Analysis}
\label{subsec:recurrence_analysis_methods}

Recurrence Plots are generated by thresholding the pairwise self-distance matrix of a trajectory computed using each metric:
\begin{equation}
    R_{i,j}(\epsilon) = \Theta(\epsilon - d(\mathbf{x}_i, \mathbf{x}_j)),
\end{equation}
where $\epsilon$ is a recurrence threshold and $\Theta$ is the Heaviside function. The resulting binary matrix visualizes when the system revisits similar states in its embedding space.

We analyze the plots qualitatively and quantitatively using Recurrence Quantification Analysis (RQA) metrics, including:
\begin{itemize}
    \item \textbf{Determinism (DET):} The fraction of recurrence points forming diagonal lines, indicating predictability.
    \item \textbf{Laminarity (LAM):} The percentage of recurrence points forming vertical lines, indicating periods of stable or "laminar" states.
    \item \textbf{Entropy (ENT):} The Shannon entropy of the distribution of diagonal line lengths, reflecting the complexity of the deterministic structure.
    \item \textbf{Recurrence Rate (RR):} The percentage of recurrent points.

\end{itemize} % 

% G: if i could say something about choice of threshold that would be great. for the RP images not so necessary but in RQA part yes

\subsection{Dimensionality}
\label{subsec:dimensionality_methods}
We estimate the correlation dimension ($D_2$) of the trajectories using the Grassberger-Procaccia algorithm \cite{GRASSBERGER1983189} \footnote{Other methods, such as box dimension or Hausdorff dimension, are less robust and harder to compute in higher dimensions}. It is derived from the scaling of the correlation integral, $C(\epsilon)$, which measures the probability that two points on the attractor are closer than a distance $\epsilon$. For small $\epsilon$, the correlation integral scales as a power law:
\begin{equation}
    C(\epsilon) = \lim_{N \to \infty} \frac{1}{N(N-1)} \sum_{i \ne j} \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|) \sim \epsilon^{D_2}
\end{equation}
where $N$ is the number of points on the trajectory and $\Theta$ is the Heaviside step function. The dimension $D_2$ is then found as the slope of the linear region in a log-log plot of $C(\epsilon)$ versus $\epsilon$. The correlation integral can be caluclated by thresholding the distance matrix, which makes both similar.

\subsection{Experiments}
\label{subsec:experiments}
% G: conretely explain what I did: model choice, params, dim, fix length generation, context window, etc...

For this study, we used the following experimental setup:
\begin{itemize}
    \item \textbf{Model:} deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \cite{deepseek2025r1distillqwen1.5b} \footnote{The model used is distilled from DeepSeek R1 reducing the number of parameters from 671 billion to 1.5 billion. It is trained and fine-tuned to replicate the behavior from the DeepSeek R1. This distilled model is based on the Qwen2.5 family and there are differences in the architecture, which are beyond the scope of this study. All the points mentioned in Section \ref{subsec:llms} apply to both, and generally to most LLMs. The reason we used this model was memory limitations in the GPUs available for this study.}
    \item \textbf{Temperature:} 0 (to ensure determinism)
    \item \textbf{Sampling:} greedy (to ensure determinism)
    \item \textbf{Context Window:} 3096 tokens
    \item \textbf{Initial Perturbation Magnitude:} 0.00035 or 0.0004 
    \item \textbf{Vocabulary Size:} 151936
    \item \textbf{Hidden/Embedding Dimension:} 1536
    \item \textbf{Attention Heads:} 12
    \item \textbf{Hidden Layers:} 28
\end{itemize}

We use the model for inference (text generation) on a Tesla T4 GPU, and we obtain the generated tokens, text, and hidden states at all the layers, i.e., the representations after each transformer block \footnote{The shape of the complete hidden state tensor for a given response is (number of layers, number of tokens, hidden dimension)}. A particular feature of the model used, trained on DeepSeek R1, is that it uses Chain-of-Thought reasoning at the start of every answer \cite{deepseekR1}. Unless explicitly stated, we use greedy sampling, except in some cases, where in order to compare with the deterministic case we use probabilistic sampling with $T=0.6$, $\text{top}_p = 0.95$ and $\text{top}_k = 50$ \footnote{This configuration means that the model samples the tokens using Softmax with $T=0.6$ and considers the minimum between $\text{top}_k$ tokens or as many tokens needed for their probability to cover $\text{top}_p$. We used these values as these are the default ones for this model. Probabilistic sampling is typically used because it makes models more expressive, less repetitive and more creative.}.
The magnitude of the perturbation was manually determined by finding a value where the output text remained on topic while still being different. This was a narrow range (approximately between 0.00025 and 0.0005).

We did the data analysis using the hidden states and also using vector embeddings obtained from the text. For this we used open-source sentence embedders \footnote{We tried three different models: $sentence-transformers\_all-mpnet-base-v2$, $intfloat\_e5-large-v2$, and  $facebook\_contriever$. All yielded similar results with the distance metrics, and in this paper we show results using the first.}, which take a string as input and output a vector. We separated the text into sentences and embedded each \footnote{The shape of the sentence embedder tensor for a given response is (number of sentences, embedding dimension).}. Sentence embedders are explicitly trained so that the cosine similarity between the vectors is related to their semantic similarity. This method reduces the amount of data, and aggregates the meaning within each sentence, and we compare the results obtained with both approaches. 

\subsubsection{Initial Condition Generation}
\label{subsec:init_cond_gen}
To test for SIC, we generated a set of trajectories from close starting points. We begin with a single prompt, tokenize it, and retrieve its initial embedding tensor $\mathbf{E}_0 \in \mathbb{R}^{L \times D}$, where $L$ is the prompt length and $D$ is the model's hidden dimension. We then create a "blob" of $N$ perturbed initial conditions by adding a small, random direction perturbation tensor $\in \mathbb{R}^{L \times D}$ of fixed magnitude $r$. Then we use LLM to continue each trajectory. This was done for different intial prompts on different topics.

\section{Results}
\label{sec:results}

\subsection{Trajectory Divergence and Lyapunov} \label{res:lyapunov}  
% G: also better introduce this section

We measured the Lyapunov exponents as desceribed in Section \ref{subsec:methods_lyapu}. The excluded trajectories constituted 40\% of the total, which corresponds either to negative lyapunov exponents or low quality fits. From the successful fits (see in Fig. \ref{fig:example_lyapu_fit}) we obtained the lyapunov exponent for 2969 pairs. The average $r^2$ of the fits was 0.905.


% % It suggests the existence of stable regions or fixed points in the state space. Even in a chaotic system, not all initial conditions lead to divergence. Some might lie on stable manifolds. The fact that ~10% of trajectories are stable is a quantifiable measure of the system's stability. 
% % G: and also it could be the fault of using random direction perturbation and two of them being close randomly. mentioned in subsec:appendix_limitations

\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\captionof{table}{Lyapunov Exponents}
\begin{tabular}{lc}
\hline
Mean & 0.32 \\
Median & 0.20 \\
Std & 0.24 \\
Min & 0.06 \\
Max & 0.72 \\
\hline
\end{tabular}
\label{table:lyap}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{plots/lyapu/fit_5_7.png}
\caption{Exemplary exponential fit using hidden states and Cosine Distance}
\label{fig:example_lyapu_fit}
\end{minipage}
\end{figure}


% todo: \label{table:lyap} and \label{fig:example_lyapu_fit} put them side by side

We repeated this same process on the sentence embedding data. 33\% of pairs were excluded and the average $r^2$ of the fits was 0.97. The values for the lyapunov spectrum obtained are different from the ones obtained in the hidden state space, which is completely reasonable, as one refers to the token space, where the "time interval" is 1 token, and the other refers to the sentence space and the "time interval" is 1 sentence. Both show and exponential divergence and a positive maximum lyapunov exponent, indicating SIC.

\begin{figure}[H]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\captionof{table}{Lyapunov Exponents}
\begin{tabular}{lc}
\hline
Mean & 0.16 \\
Median & 0.12 \\
Std & 0.079 \\
Min & 0.038 \\
Max & 0.29 \\
\hline
\end{tabular}
\label{table:lyap_embed}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{plots/lyapu/fit_46_48.png}
\caption{Exemplary exponential fit using sentence embedders and Cosine Distance}
\label{fig:example_lyapu_fit_embed}
\end{minipage}
\end{figure}


We also calculated the divergence using other distance metrics. We see that the trajectories remain extremely close for several steps until they suddenly diverge. In the text, the first words are the same until, at the diverging step, the words turn different and the sentences too, while the general topic remains the same. So the initial perturbation persists in the hidden states, but does not affect token selection until a certain step, where the sampled token by two trajectories is different, which then leads them to quickly diverge.
The explanation for this is that at each step, the most probable next token is selected. If the perturbation is not large enough, at a given step it will not change the selected token, which keeps both trajectories close. In the text space, and in the sentence embeddings, the perturbation will not be detectable at all, while in the hidden state space, it will remain. After some steps, it might occur that the perturbation, which propagates and possibly amplifies due to the non-linear coupling, induces a different next token to be selected. This causes a big discrete difference with the non-disturbed trajectory, which then strongly influences the next token prediction. This causes trajectories to rapidly diverge once the first different token is predicted.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/cross_corr_pearson_0_1.png}
        \caption{Cross-Correlation Dissimilarity}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/dtw_timeseries_0_1.png}
        \caption{DTW}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/frechet_timeseries_0_1.png}
        \caption{Fréchet}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_layer22/hausdorff_timeseries_0_1.png}
        \caption{Haussdorf}
    \end{subfigure}
    \caption{Divergence between a pair of trajectories from hidden states using different metrics.}
    \label{fig:distance_hidden}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/cross_corr_pearson_0_1.png}
        \caption{Cross-Correlation Dissimilarity}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/dtw_timeseries_0_1.png}
        \caption{DTW}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/frechet_timeseries_0_1.png}
        \caption{Fréchet}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_1.png}
        \caption{Haussdorf}
    \end{subfigure}
    \caption{Divergence between a pair of trajectories from sentence embeddings using different metrics.}
    \label{fig:distance_embed}
\end{figure}

Figures \ref{fig:distance_hidden} and \ref{fig:distance_embed} show that considering trajectories of hidden states or sentence embeddings, the divergence curves are similar. Some metrics show a discrete, almost step-function-like divergence, like Fréchet in the hidden state space. % 

% It is clear that we do not see an exponential divergence between the trajectories with most metrics, but rather a sudden jump.
% Some metrics are better at quantifying the divergence, like cross correlation, DTW and Hausdorff, while others, like Fréchet are worse. %  improve
% In our case, especially when using sentence embeddings, the number of data points until the saturation region was often small (G around 6), making any type of curve fitting difficult.


\subsection{RP}
\label{subsec:rp_results}

Plotting the RPs for trajectories coming from different prompts shows a wide variety of results. Strong similarities can be seen between the RP from the Lorenz \cite{DeterministicNonperiodicFlow} system as seen in Fig. \ref{fig:rp_examples} and some of the RPs obtained from the LLMs hidden states. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/interstellar_propulsion_review_recurrence_first_thr_0.3.png}
        \caption{Prompt 1, first layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/interstellar_propulsion_review_recurrence_last_thr_0.3.png}
        \caption{Prompt 1, last layer}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/quantum_consciousness_hallucination_recurrence_first_thr_0.3.png}
        \caption{Prompt 2, first layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/quantum_consciousness_hallucination_recurrence_last_thr_0.3.png}
        \caption{Prompt 2, last layer}
    \end{subfigure}
    \caption{Recurrence plots for different prompts and layers using Cosine Distance.}
    \label{fig:rp_comparison_layers}           
\end{figure}


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
Name & Recurrence Rate & Determinism & Longest Diagonal Line & Entropy of Diagonal Lines & Laminarity & Longest Vertical Line & Entropy of Vertical Lines \\
\midrule
Prompt 1, first layer & 0.0104 & 0.4097 & 93.0000 & 1.3277 & 0.0160 & 2.0000 & 0.0000 \\
Prompt 1, last layer & 0.0137 & 0.4247 & 72.0000 & 1.1857 & 0.1336 & 5.0000 & 0.5429 \\
Prompt 2, first layer & 0.0127 & 0.2941 & 23.0000 & 1.3155 & 0.0000 & 0.0000 & 0.0000 \\
Prompt 2, last layer & 0.0139 & 0.4174 & 24.0000 & 1.2225 & 0.1929 & 11.0000 & 0.4682 \\
\bottomrule
\end{tabular}%
}
\caption{Recurrence quantification analysis results.}
\label{tab:rqa}
\end{table}


% G: not always at the last layer, but also middle layers are quite chaotic
Chaotic features are more strongly present at the last layers of the model as showed in the RP \ref{fig:rp_comparison_layers} and pointwise dimension \ref{fig:dim_first_last}. This is due to the progressive addition of more nonlinearity and coupling at each layer. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.04/traj_2_recurrence_thr_0.3.png}
        \caption{Prompt a}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.04/traj_3_recurrence_thr_0.3.png}
        \caption{Prompt b}
    \end{subfigure}
    \\[0.5em]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.02/traj_1_recurrence_thr_0.3.png}
        \caption{Prompt c}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/RP/launch_pentek_0_0.02/traj_6_recurrence_thr_0.3.png}
        \caption{Prompt d}
    \end{subfigure}
    \caption{Examples of recurrence plots obtained for different prompts.}
    \label{fig:rp_interesting}
\end{figure}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
Name & Recurrence Rate & Determinism & Longest Diagonal Line & Entropy of Diagonal Lines & Laminarity & Longest Vertical Line & Entropy of Vertical Lines \\
\midrule
Prompt a & 0.0498 & 0.4341 & 13.0000 & 1.2627 & 0.4472 & 17.0000 & 1.2769 \\
Prompt b & 0.0447 & 0.5407 & 88.0000 & 1.8077 & 0.1203 & 13.0000 & 0.5389 \\
Prompt c & 0.0315 & 0.4875 & 170.0000 & 1.2416 & 0.3485 & 12.0000 & 0.9297 \\
Prompt d & 0.0260 & 0.4986 & 407.0000 & 1.5316 & 0.2421 & 16.0000 & 0.3053 \\
\bottomrule
\end{tabular}%
}
\caption{Recurrence quantification analysis results.}
\label{tab:rqa2}
\end{table}

The Chain-of-Thought reasoning block, the first tokens generated by the model, can be identified in the RP (dark left bottom block). It suggests the model enters a specific, stable 'reasoning state' before beginning its main output. This is a great example of using RPs to identify functional stages in the generation process. 
The blocks in the RP, such as in Figure \ref{fig:rp_interesting} (b) correspond to some structured parts of the text, such as making lists.


\subsection{Dimensionality}

Figure \ref{fig:dim_first_last} shows the Correlation Sum for a random set of vectors, and for the points in a trajectory using the hidden states from the first and last layers of the LLM. The curve obtained for the last layer is what we would expect from a system with low-dimensional chaos. The measured dimension value obtained varied significantly depending on the distance metric used, while in every case, the Correlation Dimension vs Threshold curve was almost identical, but with a different slope. Therefore we don't consider this as a precise measure of the fractal dimension of an attractor, but as an indicator of deterministic chaos, which helps us identify differences between layers, and between deterministic and nondeterministic sampling.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/random.png}
        \caption{Random}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_first_first.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_last_last.png}
        \caption{Last layer}
    \end{subfigure}

    \caption{Correlation dimension plots for a random set of vectors, and the first and last layers of the LLM.}
    \label{fig:dim_first_last}
\end{figure}

In Figure \ref{fig:dim_first_last} note that the random set of vector's correlation sum resembles the first layer's close to a threshold value of around 0.9 and above, which can more clearly be seen in Figure \ref{fig:distribution_cos_dist}. In the first layer distance distribution, a peak at 1 is observed, which corresponds to almost orthogonal vectors. For a set of random vectors of the same dimension, we see a more narrow distribution peaked at 1 \footnote{The number of almost orthogonal vectors grows exponentially with the dimension of the space \cite{cai2013distributionsanglesrandompacking}. In LLMs the curse of dimensionality can be a blessing as they can represent many more concepts than the embedding dimension \cite{blessing}. Empirical results show that LLMs represent high-level concepts linearly as directions in space \cite{park2024linearrepresentationhypothesisgeometry}, with separable concepts being almost orthogonal.}.
In the first layer are many vectors with distance equal to 0, which correspond to repeated tokens. These are not as prominent in the last layer, because after going through all the transformer layers, same tokens get mapped to different hidden states at the last layer, as the mapping is state dependent. % G: could be more concise
All of the cases analyzed showed two main peaks at values close to 0.6 and 1, for the distribution at the last layer, although not in every case was the peak at 0.6 so pronounced.
% any explanation for this

% random at first layer is because it doesn't have all the info of the context so it appears random.
% only at last layers can you see the true chaoicity
%  A non-integer value for Dimenion is a hallmark of a fractal strange attractor. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/random_dist.png}
        \caption{Random}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_first_first_dist.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/childhood_personality_development/cosine_sim_last_last_dist.png}
        \caption{Last layer}
    \end{subfigure}

    \caption{Distribution of cosine distance for a random set of vectors, and the first and last layers of the LLM.}
    % \caption{Distribution of cosine distance for the first and last layers of the LLM. The top and bottom rows are from different prompts}
    \label{fig:distribution_cos_dist}
\end{figure}

The first layer hidden states show more resemblance with the randomly generated set of vectors, with the main difference that the former have many repeated tokens, which lead to many equal vectors. This corresponds to the almost constant correlation sum for low threshold values (see Fig. \ref{fig:dim_first_last}), which constrasts to the linear increase in the last layer hidden state case. For the first layer hidden states and the random vectors, the main increase in the correlation sum is close to the threshold = 1, corresponding to the single peak in the distance distribution. In the first layer, the Correlation Sum and the distribution is not as continuous as these vectors are discrete and taken from the embedding matrix, and do not approximate a continuous manifold.

% The correlation dimension of a random set of vectors tends to the embedding dimension. 

% In the cases investigated the correlation dimension obtained using the L2 distance metric was aproximately twice the one obtained using cosine similarity.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/temperature06_childhood/cosine_sim_first_first_T06.png}
        \caption{First layer}
    \end{subfigure}\hfill
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/temperature06_childhood/cosine_sim_last_last_T06.png}
        \caption{Last layer}
    \end{subfigure}\hfill

    \caption{Correlation dimension plots for the first and last layers of the LLM using probabilistic sampling with $T = 0.6$.}
    \label{fig:dim_first_last_temperature}
\end{figure}

Finally in Figure \ref{fig:dim_first_last_temperature} we show the Correlation Sum obtained from a trajectory using probabilistic sampling, meaning that there is some stochaticity at the selection of each token. We see almost no difference at the first layer, compared to the deterministic case, but at the last layer we clearly see different behavior. There is clearly no clear linear region to extract a dimension value.
The fact that the difference between the deterministic and probabilistic trajectories can be ideantified only at the last layer supports the idea that chaotic behaviour is experienced at the last layer.

% G: it would be great to show if last layers have a lower dimension, which could imply more "compression" of the data
% G: try to relate the dimension here with information compression, which is something super important that LLMs do.  

\section{Conclusions and Discussion}
\label{sec:conclusions}

% (rough) invariance for different metrics {hausdorf, frechet, dtw} (and also embedders). discuss. they literally look the same. I mean to be fair the data is the same. quite trivial
% It's a sign of robustness. These metrics measure very different geometric properties. The fact that they all show the same qualitative behavior is evidence that we are observing a real dynamical phenomenon, not an artifact of your measurement tool.

This study explored the internal dynamics of a Large Language Model (LLM) through the lens of nonlinear dynamical systems, with particular focus on \textbf{chaoticity in the embedding space}. By analyzing trajectories of hidden states and their semantic equivalents, we investigated whether the evolution of representations within an autoregressive Transformer can exhibit behaviors analogous to deterministic chaos.

Our experiments revealed consistent and robust patterns of divergence and recurrence across multiple metrics and representational levels. Regardless of the distance measure employed (e.g., Hausdorff, Fréchet, DTW) or the representational space (hidden states vs. sentence embeddings), the qualitative behavior of the trajectories remained remarkably similar. This convergence across fundamentally different geometric measures suggests that the observed effects are not artifacts of the chosen metric, but rather reflect genuine structural properties of the model's internal dynamics.

Initially, we hypothesized that the state of the system could be well described by the hidden states at the last layer. This seemed like a reasonable assumption given that the next token is predicted exclusively from the current token's last hidden state, 
%by applying the unembedding matrix and softmax.
and that at the last layer, the hidden states could incorporate much of the meaning and context via the transformer layers.
This assumption proved to be too simplistic, as measuring the distance between trajectories (where trajectories were considered the sequence of hidden states at the last layer) gave very noisy curves that did not properly reflect the semantic distance evolution.
This motivated the use of techniques such as sliding windows, sentence embedders and other distance metrics, which all consider the state as described not by individual tokens, but by a window of them, analogous to a delay system. This proved to be successful. From the LLM architecture point of view, this is sensible as the mapping of the model depends on all the tokens in its context window. 

The results obtained from the divergence using hidden state and sentence embedders were extremely similar. This, and the fact that the distance metrics when applied to the hidden states yield meaningful results (when compared to semantic similarity), is not trivial, as LLMs are not trained to generate interpretable hidden states whose distance encodes meaning. Sentence embedders are models explicitly trained for this, and it is remarkable that both achive such similar results.

From a dynamical perspective, our results support the interpretation of LLMs as high-dimensional, discrete-time nonlinear systems operating near the \textbf{edge of chaos}. The combination of the self-attention mechanism (which induces stretching through context-dependent amplification of differences) and normalization layers (which impose folding through bounded projection) produces a balance between divergence and confinement analogous to chaotic attractors. The positive lyapunov exponent shows sensitivity to initial conditions. The recurrence plots exhibit structured yet non-periodic geometries, while dimensionality estimation suggests that these trajectories evolve on a low-dimensional fractal manifold rather than filling the entire embedding space. Together, these findings point toward a regime of rich but bounded dynamics—a hallmark of systems capable of both stability and flexibility.

% This behavior may underlie the dual ability of LLMs to maintain coherence while generating diverse, creative continuations. Systems at the edge of chaos are known to maximize computational expressivity and information propagation, suggesting that such dynamics could play a role in the emergent reasoning and context sensitivity observed in large-scale Transformers.

Working in extremely high-dimensional spaces presented both conceptual and technical challenges. Classical chaos analysis techniques—such as Lyapunov estimation and fractal-dimension scaling—were not directly applicable without adaptation. Our experiments highlight the importance of selecting appropriate state representations: a single hidden vector does not fully capture the model's instantaneous state, as the LLM's transition function inherently depends on its context. The success of sliding-window and embedding-based analyses underscores the view that an LLM is best modeled as a \textbf{delay system}, where state evolution depends on a finite history of tokens. 
%This connection provides a valuable bridge between the mathematical theory of complex systems and modern language model architectures.

Our investigations were limited to studying a single LLM model. Nontheless, we expect similar results in the case of other models and architectures as well, but with some differences, since certain architectural features of the model, such as the Chain-of-Thought reasoning logic, could be recognized in the recurrence plots. Further studies should explore different model sizes, architectures, and context sizes.
Future work could also systematically explore how chaotic characteristics scale with model size, architecture, and training regime. Comparing encoder-only, decoder-only, and encoder–decoder architectures could illuminate how bidirectional vs. causal attention affects dynamical stability. Moreover, investigating the influence of hyperparameters such as context length, normalization type, and activation function could clarify which architectural components are the main cause of chaotic dynamics.

% Another question that arose during our studies involves measuring Lyapunov-like exponents directly from embedding trajectories, potentially revealing quantitative thresholds for transitions between stable and chaotic regimes. Extending these analyses to stochastic decoding at nonzero temperatures would allow for exploration of the interplay between deterministic chaoticity and sampling-induced randomness.

% Finally, reinterpreting the trajectory could yield further insight in the inner workings of reasoning LLMs. Namely, one could define a trajectory by checking which nodes in the subsequent layers are activated. This is also known as tracing the system for a given input.



% mention the challenges and successes with high dim data. also distance metrics. and the ones I invented
% "Our results paint a consistent picture of LLM dynamics. We observe sensitive dependence on initial conditions, characteristic of chaos. Recurrence plots reveal complex, deterministic structures that evolve through the network's layers, culminating in chaotic-like behavior in the final layers. Furthermore, dimensionality analysis suggests these complex dynamics unfold on a low-dimensional fractal attractor. This suggests that LLMs operate at the 'edge of chaos', leveraging these rich dynamics to balance creativity and coherence. We propose that the interplay between the 'stretching' of the attention mechanism and the 'folding' of normalization layers is the fundamental architectural driver of this behavior."



% G: Try to frame some of the challenges and difficulties as interesting things
% - finding a good distance metric is tricky. it seems like each token (and its hidden state) is representing mainly that token's meaning, and not the whole state of the system (or trajectory) up until that point. In order to really describe the state of the system, we need the previous tokens too (like in the case of a delay system in chaos and nonlinear systems). The usage of sliding windows and/or sentence embedder for the distance metrics does something like this (capturing local context), and the fact that it makes the distance curves much better shows how important the context is
% this point is quite imporatnt for divergence, but not so necessary (or convenient) for RP
% empirically from the data analysis the LLM is better modeled as a delay system. The 'state' is not just the current hidden vector, but a window of past vectors. This justifies sliding window approach and connects LLM dynamics to a well-understood class of complex systems.

\section{Acknowledgments }
\label{sec:acknowledgements}

I would like to thank my supervisors Dr. János Török and Kristóf Benedek for their help and support during this work. This work was supported by the National Research, Development and Innovation Fund and by the Ministry of Culture and Innovation of Hungary through the TKP2021-NVA-02 OR TKP2021-EGA-02 funding scheme.

\newpage
\printbibliography


\section{Appendix}
\label{sec:appendix}

\subsection{Intuition behind different metrics}
\label{subsec:appendix_intuition_metrics}

In order to gain a more intuitive understanding of each metric, and the effect of the sentence embedders vs the raw hidden states, we show the RPs from the same trajectory using different metrics both with the hidden states and with the sentence embeddings.

% G: maybe use cross_corr instead of cross_cos as cross_corr is bettter in distance timeseries
\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/hausdorff_traj0.png}
        \caption{Hausdorff}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/dtw_fast_traj0.png}
        \caption{DTW}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/frechet_traj0.png}
        \caption{Fréchet}
    \end{subfigure}
    % Second row
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/rank_eigen_traj0.png}
        \caption{PCD}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/cos_traj0.png}
        \caption{Cosine Distance}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/hidden/cross_cos_traj0.png}
        \caption{Cross-Correlation Dissimilarity}
    \end{subfigure}
    \caption{Distance matrices for a single trajectory using different metrics and using the hidden states.}
    \label{fig:distance_metrics_comparison_hidden}
\end{figure}

In Figure \ref{fig:distance_metrics_comparison_hidden} we see that Fréchet (c) and Cosine Distance (e) do not recognize the diagonal lines in the hidden state space, but, when applied on sentence embeddings (Fig. \ref{fig:distance_metrics_comparison_embed}) they do.

\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/hausdorff_traj0.png}
        \caption{Hausdorff}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/dtw_fast_traj0.png}
        \caption{DTW}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/frechet_traj0.png}
        \caption{Fréchet}
    \end{subfigure}
    % Second row
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/rank_eigen_traj0.png}
        \caption{PCD}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/cos_traj0.png}
        \caption{Cosine Distance}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/distance_matrix/embed/cross_cos_traj0.png}
        \caption{Cross-Correlation Dissimilarity}
    \end{subfigure}
    \caption{Distance matrices for a single trajectory using different metrics and using the text embeddings.}
    \label{fig:distance_metrics_comparison_embed}
\end{figure}

It can also be seen how the use of sentence embedders helps identify deterministic and recurring themes, which are not evident at the hidden state level.


% G: Here i want to show how embeddings kind of do some averaging and pooling and context, and how some metrics pick up different features, some more global some more detail. 
% G: mention window size


\subsection{Use of Sliding Window and Sentence Embeddings} 
\label{subsec:sliding_window}

\begin{figure}[H]
    \centering
    % First row: Hidden state space
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_1.png}
        \caption{Hidden state, window size 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_8.png}
        \caption{Hidden state, window size 8}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_last_layer/hausdorff_timeseries_0_2_window_size_16.png}
        \caption{Hidden state, window size 16}
    \end{subfigure}
    % Second row: Sentence embedding space
    \\[0.5em]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_1.png}
        \caption{Embedding, window size 1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_8.png}
        \caption{Embedding, window size 8}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/divergence/sliding_window_compare/childhood_personality_development_0_0.0004_embed/hausdorff_timeseries_0_2_window_size_16.png}
        \caption{Embedding, window size 16}
    \end{subfigure}
    \caption{Divergence between two trajectories using Hausdorff distance, for different sliding window sizes. Top row: hidden state space. Bottom row: sentence embedding space.}
    \label{fig:sliding_window_compare}
\end{figure}

We see that in the hidden state space, without a sliding window, the distance measurement is very noisy, which is not as pronounced when using the sentence embedders. This shows that the hidden states represent mainly the meaning of the token they are associated with, and not so much of the context (even though they have access to information from other tokens via attention). 
We were originally expecting the hidden states to represent the whole state of the system up to that point, but this turned out to be false, and considering the hidden states in the context window is also necessary. This makes LLMs more similar to a delay system. % G: 
% G: With window size 1, the hidden state distance is noisy. As you increase the window size, a clear divergence curve emerges. This visually demonstrates that the 'true' state of the system is a sequence of vectors, not a single vector. connect size of window with semantic


% %  sliding window can make step function look linear diagonal with cos and dtw. maybe delete section
% % G: also maybe try cos with min aggregation (do not average the vectors fist, but first calc each pair and then min)
% Some artifacts can be introduced by the sliding windows. For some metrics the distance gets averaged over the window and this just turn a step function-like curve to a piecewise linear looking one.
% Other metrics, such as Hausdorff, which calculate the maximum of the minimum distance, do not present this problem, since they do not average out the distance.
% \begin{figure}[H]
%     \centering
%     % First row: Hidden state space
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_dtw/dtw_timeseries_0_1_window_size_1.png}
%         \caption{window size 1}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_dtw/dtw_timeseries_0_1_window_size_8.png}
%         \caption{window size 8}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_dtw/dtw_timeseries_0_1_window_size_16.png}
%         \caption{window size 16}
%     \end{subfigure}
%     % Second row: Sentence embedding space
%     \\[0.5em]
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_cos/cos_timeseries_0_1_window_size_1.png}
%         \caption{window size 1}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_cos/cos_timeseries_0_1_window_size_8.png}
%         \caption{window size 8}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots/divergence/interstellar_propulsion_review_0_0.00035_sentence_transformers_cos/cos_timeseries_0_1_window_size_16.png}
%         \caption{window size 16}
%     \end{subfigure}
%     \caption{Divergence between two trajectories using sentenec embedders for different sliding window sizes. Top row: DTW. Bottom row: Cosine Distance.}
%     \label{fig:sliding_window_artifact}
% \end{figure}


\subsection{Dimensionality of the LLM vocabulary}
\label{subsec:appendix_vocab_dim}


We also did this analysis on the embedding and unembedding matrix, which for the LLM model we use is the same. This matrix represents the vocabulary of the model, and it is not a trajectory. 
The reason we did this is because of the large number of data points available (around 150 thousand) without requiring any computation, and that it could provide insights into the structure of the vocabulary space. It is interesting to see that the results are different from those obtained from the analysis done on the trajectories, or what is obtained from a random set of vectors. The value obtained for the correlation dimension is smaller than expected.
% G: I don't have a good explanation or much to say here.
% G: a trajectory generated by the model is not just a random walk through this vocabulary space; the trajectory uses new vectors

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/dimension/vocab/vocab_full.png}
        % \caption{Correlation Dimension = 1.2}
    \end{subfigure}\hfill
    % \begin{subfigure}[b]{0.48\linewidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{plots/dimension/vocab/vocab_last.png}
    %     \caption{Correlation Dimension = 5.4}
    % \end{subfigure}
    \caption{Correlation dimension of the LLM's embedding matrix.}
    \label{fig:embedding_dim}
\end{figure}


\subsection{Layer normalization (G: ignore for now, this is bad)}\label{subsec:layer_norm}% G: this is quite bad currently, I'll make it shorter and more relevant.
% G: mention other normalizations? like batch?

Layer normalization (LayerNorm) acts on a single hidden state vector \(h\in\mathbb{R}^d\) (e.g. a token embedding) by (i) mean-centering, (ii) variance normalization, and (iii) a learned affine anisotropic rescaling.
\begin{align}
\mu(h) &\;=\; \frac{1}{d}\sum_{i=1}^d h_i, \\
\sigma^2(h) &\;=\; \frac{1}{d}\sum_{i=1}^d (h_i-\mu(h))^2, \\
\widehat{h} &\;=\; \frac{h - \mu(h)\mathbf{1}}{\sqrt{\sigma^2(h)+\varepsilon}}, \label{eq:ln_norm}\\
y &\;=\; \gamma \odot \widehat{h} + \beta \;=\; D\widehat{h} + \beta, \label{eq:ln_affine}
\end{align}
where \(\mathbf{1}=(1,\dots,1)^\top , \gamma, \beta \in\mathbb{R}^d\), and \(D=\operatorname{diag}(\gamma)\).
% G: pretty unreadable. 
% G: benefits:
% Introducing a normalization of the hidden states after every operation as part of the model architecture made th training between 4 and 20 times faster \cite{loshchilov2025ngptnormalizedtransformerrepresentation}
% Representation learning on the hypersphere leads to more stable training, greater embedding space separability, and better performance on downstream tasks \cite{wang2022understandingcontrastiverepresentationlearning}
% Normalization techniques are beneficial \cite{salimans2016weightnormalizationsimplereparameterization}

% G: maybe we can hypothesize whether norm keeps the model more stable by limiting lyapunov exponents and keeping it at the edge of chaos

\begin{itemize}
  \item \(h\in\mathbb{R}^d\): input hidden state (one token, one sample).
  \item \(\mu(h)\): coordinate mean of \(h\)
  \item \(\sigma^2(h)\): empirical coordinate variance of \(h\)
  %\item \(\widehat{h}\): normalized (zero-mean, unit-variance) vector
  \item \(\gamma\in\mathbb{R}^d\) (or \(D=\mathrm{diag}(\gamma)\in\mathbb{R}^{d\times d}\)): per-coordinate learned scale (anisotropic linear part)
  \item \(\beta\in\mathbb{R}^d\): learned bias
  \item \(\varepsilon\): numerical stabilizer
\end{itemize}

Geometrically, LayerNorm performs a series of transformations. First, the centering step projects the hidden state onto a hyperplane orthogonal to the vector $\mathbf{1}$, effectively removing the mean component. The subsequent variance normalization is a nonlinear operation that maps the centered vector to a point on a sphere of radius $\sqrt{d}$. This step reduces the degrees of freedom from $d$ to $d-2$, as the image lies on a $(d-2)$-sphere within the mean-zero hyperplane. Finally, the affine transformation scales the sphere into an ellipsoid and translates it, without mixing the coordinate axes.

\paragraph{Jacobian (local linearization)}
Let \(u(h):=h-\mu(h)\mathbf1\) and \(\|u\|=\sqrt{u^\top u}\).  A convenient form of the Jacobian of the normalization map \(\widehat{h}(h)\) is
\[
J_{\widehat{h}}(h)
\;=\;
\frac{\sqrt{d}}{\|u\|}\,\Big(I - \frac{u u^\top}{\|u\|^2}\Big)\Big(I - \frac{1}{d}\mathbf1\mathbf1^\top\Big),
\]
and the full LayerNorm Jacobian (including \(D\)) is \(J_y(h)=D\,J_{\widehat{h}}(h)\).  For generic \(h\) (with \(d>2\)) this linearization has rank \(d-2\): two null directions correspond to the mean direction \(\mathbf1\) and the radial direction \(u\) (the latter is collapsed by the radial normalization).
% G: By killing the �� u-direction, LayerNorm throws away “how much” the features deviate from the mean and keeps only “in what relative proportions” they deviate.
% G: could emphasize that the radial direction u is specific for every h, but the mean centering is global.

\paragraph{Interpretation for dynamics} %  \paragraph
% \label{par:interpretation_dynamics}
LayerNorm is therefore a structured many-to-one nonlinear map: it (i) removes one linear degree of freedom (the mean), (ii) collapses the radial coordinate in the centered hyperplane (folding onto a compact manifold), and (iii) applies an anisotropic linear rescaling and shift.
% G: (ii) is too much
% In dynamical terms this provides a strong folding/bounding operation; any stretching/expansion must come from the other components (linear layers, attention, pointwise nonlinearities) for the overall layer-to-layer map to exhibit expansion+folding behavior.
% G: I'm not sure if this is sound. why call it a folding when it is rather a projection
% G: It's both. A projection is a form of folding. It takes a larger space and maps it to a smaller one. In this case, it's a projection onto a hypersphere, which is a bounded manifold. This  acts as a folding mechanism, preventing trajectories from escaping to infinity. 

\subsection{Principal Component Dissimilarity (PCD)}
\label{subsec:pca_distance} 

% We also defined a new distance metric based on the structural similarity of two trajectories. It showed similar results to the other metrics but it was noisier. 

We perform a PCA on each trajectory obtaining the principal directions (eigenvectors $V^{(i)}$ of the centered data covariance matrices) for each trajectory. To compare the two sets of eigenvectors, we compute the cosine similarity matrix $S$.
For each eigenvector of the first trajectory $v^{(1)}_a$, we find the index $b^*$ of the most similar eigenvector in the second trajectory:
\[
b^* = \arg\max_b S_{ab}
\]
We can show the ranking of eigenvectors to show how similar two trajectories are. The closer to the proportionality line, the more similar two trajectories.

To get a distance scalar, we sum the cosine distances for these matched pairs \footnote{We also tried calculating the root mean square deviation with respect to the proportionality line but it was more noisy.}:
\[
D_{\text{sum\_cos\_dist}} = \sum_{a=1}^k \left[1 - S_{a b^*}\right]
\]
where $D_{\text{sum\_cos\_dist}}$ is the scalar metric quantifying the dissimilarity between the two trajectories.

This metric is not symmetric. Matching is performed from the eigenvectors of the first trajectory to those of the second, and the assignment is not necessarily reciprocal. We compute both and average the two.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{plots/rank_eigen/rank_eigen_pca_0_2.png}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{plots/rank_eigen/rank_eigen_pca_0_1.png}
    \end{subfigure}
    \caption{Principal Component eigenvector ranking between trajectories. The left image shows two more similar trajectories} 
    \label{fig:rank_eigen}
\end{figure}


\subsection{Ideas that Failed}
\label{subsec:appendix_failed_ideas} % G: under development. maybe wont be in paper
In this section, we briefly mention a few ideas that did not work in case it is of use to others. We used the Wasserstein metric for the data analysis but it was noisy and not very informative. We also used cross-recurrence plots to compare two trajectories, but found it not so clear. To compare different trajectories we preferred the methods explained in the paper. Inspired by \cite{DONNER_2011} and \cite{Donner_2010} \cite{ZOU20191} we did the recurrence analysis using complex networks, which comes naturally by considering the recurrence matrix as the adjacency matrix of the network. While interesting, we found the results easier to analyze using RQA than this method. % G: improve


% \subsection{Limitations and shortcomings of this study (GGG this won't be in the paper in this form, I just want to keep track of these):}
% \label{subsec:appendix_limitations}

% - i can't explain what the value of the dimension obtained *means*. if the dimensionality thing just shows that the system is deterministic, then it is quite trivial
% - i only qualitatively compared the recurrence plots obtained
% - i failed at aggregating results from pairs of trajectories (so far)
% - the recurrence plots for different prompts are quite different and not all look chaotic
% % G: bug or finding? It suggests the dynamics are prompt-dependent. Some prompts might induce more stable, predictable behavior, while others induce more chaotic, creative behavior. 
% - i only did english prompts
% - RQA done on different layers does not show enormous differences (although i claim that the last layers are more chaotic than the first layer). specifically
% - when i generate the initial conditions I was a bit stupid and just added a fixed-magnitude random-direction perturbation. So I did not really measure the initial distance between trajectories, and i did not really measure it. I should have done a hyper equilateral triangle, but i did a hyper sphere.
% - i did not really show a strange attractor, i just hint at it. i could not quantitatively compare the results for different prompts (or even for the same prompt but disturbed) in the RP and dimension stuff. One might ask: if you say that you get a sort of strange attractor, is it the same for different prompts? is there any similarity between them?. My answer would be: i dont know
% - relatively short trajectories:
% Using a model with so few parameters and having to limit the context window size significantly, it was difficult to make long 
% Due to memory constraints we had in the GPUs available, there was a tradeoff between the length of the generated text, and the model size and context window. This meant that we either had to do short responses (~3096 tokens long GGG), or if we wanted longer ones, we needed to use a smaller model or a smaller context window, which then caused the output quality to degrade, often resulting in the model getting stuck in a repetitive loop. The fact that the temperature was set to zero did not help.
% What we did is use the smallest model and context window size possible while retaining a good quality output text by reading through them \footnote{also recurrence plots at a glance show if a model gets stuck in a repetitive loop}. 
% We did not think this repetitive loops were worth exploring, since these are a feature which appears in very basic and small LLMs with a short context window and zero temperature, and state of the art models will overcome this.


\end{document}




% extra ideas:
% why do we have a period of time with no divergence and then suddenly a big jump in the divergence between trajectories. I think it's mainly a measurement thing: the distance metrics (and the "state vectors" are hypersensitive to the individual token, so when the first different token arises, huge jumo in divergence.
% J idea: you nead a "critical mass" of context before you can get this 
% the reasoning partbefore the answer is less suceptible to divergence. the llm kind of repeats what the prompt contains and is more predictable. G: maybe the fact that RL was used to train this reasoning part made it more constrained and less diverse.




\documentclass[a4paper,12pt]{article}
\usepackage{graphicx,amssymb,textcase,fancyhdr,enumerate,wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Use T1 encoding
\usepackage{mathptmx} % Use Times New Roman font
\usepackage[margin=2.5cm]{geometry} % Set margins to 2.5 cm
\usepackage{setspace} % For line spacing
\setstretch{1.5} % Set line spacing to 1.5
\usepackage{subfig}
\usepackage{icomma}
\usepackage{xcolor}
\usepackage{bm}
\usepackage[unicode,colorlinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=green,      
    urlcolor=blue,
}

\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{mathtools}
\usepackage[normalem]{ulem}

\usepackage{subcaption}
\usepackage{amsmath} % for the cases environment
\usepackage{amsfonts} % additional math symbols (if needed)
\usepackage{graphicx}

\DeclarePairedDelimiter\abs{|}{|}
\DeclareMathOperator{\tg}{tg}
\definecolor{new_red}{HTML}{ff0000}
\definecolor{new_blue}{HTML}{0000ff}
\definecolor{new_green}{HTML}{009a00}
\definecolor{new_orange}{HTML}{ff8000}

\colorlet{light_red}{new_red!20!white}
\colorlet{light_blue}{new_blue!20!white}
\colorlet{light_green}{new_green!20!white}
\colorlet{light_orange}{new_orange!20!white}
\newcommand{\JT}[1]{{\color{blue} #1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vv}{\vect{v}}
\newcommand{\vu}{\vect{u}}

\newcommand{\hlb}[1]{\mathpalette\highlightb{#1}}
\newcommand{\highlightb}[2]{\colorbox{light_blue}{$#1#2$}}

\newcommand{\hlr}[1]{\mathpalette\highlightr{#1}}
\newcommand{\highlightr}[2]{\colorbox{light_red}{$#1#2$}}

\newcommand{\hlo}[1]{\mathpalette\highlighto{#1}}
\newcommand{\highlighto}[2]{\colorbox{light_orange}{$#1#2$}}

\newcommand{\hlg}[1]{\mathpalette\highlightg{#1}}
\newcommand{\highlightg}[2]{\colorbox{light_green}{$#1#2$}}

\setlength{\parskip}{0pt plus 0pt}
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.1}

\sloppy


\begin{document}

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\huge\textsc{TDK}
\HRule \\[0.4cm]
{ \huge \bfseries Chaos in Large Language Models}\\[0.2cm] 
\HRule \\[0.5cm]
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
Author: \\
\Large\textbf{Jaca Gregorio}\\
\large{Physicist Engineering BSc, III. year.}
\end{flushleft}
\end{minipage}
\qquad
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft}\large
Supervisors: \\
\Large\textbf{Dr. Török János}\\
\large Associate professor \\
BME Elméleti Fizika Tanszék

\Large\textbf{Kristóf Benedek}\\
\large ? \\
?
\end{flushleft}
\end{minipage}
\\[6cm]
\includegraphics[scale=0.7]{bme.png}\\[0.2cm]
\large{\textbf{BME}}\\
\large{\textbf{2025}}
\vfill
\end{titlepage}

\newpage\null\thispagestyle{empty}\newpage

%%% --------- GUIDELINES ---------- %%%
% This doc is in progress. I write but leave many comments to remind myself where to go back and improve. Wherever you see a "(GGG)" it means that I'm not happy with the wording and it needs to be made more precise and clear.
% When I use "G:" i am straightforwardly expressing an opinion 
% It's easier for me if you use your initials before your comments, so i can quickly understand who they come from


\begin{abstract}
\end{abstract}
\newpage
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\newpage

\section{Introduction and Motivation}

% G: This paragraph wants to say: Mathematicians study silly nonlinear diff eqs for fun. LLMs are even more interesting.
Deep Neural Networks (DNNs), such as LLMs, are highly nonlinear systems whose parameters are not explicitly determined, but rather they are determined by training on huge sets of data.
% but rather they are trained on large sets of data. 
This allows these models to learn huge amounts of knowledge with little human intervention, which allows for escalation at the cost of interpretability.
DNNs provide a facsinating case of study for nonlinear dynamics.
% Studying these systems is extremely interesting

There is growing interest in the sensitivity of LLMs (and other generative deep neural networks) to initial conditions and small perturbations. For some applications, like Reinforcement Learning (RL) (G: or in cases where there is one right answer) (GGG), determinism and predictability (and reproducibility) are desireable, while for other uses like synthetic data generation, paraphrasing, and novel image generation, divergence (GGG) is desired.
% Understanding fixed/stable points in LLMs can clarify their generative capability constraints.
% G: needs a more natural phrasing, not forcing it into the SIC chaos direction until the end

% G: I want to say: LLMs are trained on all human data. what they learn is representative of human civilization (if we consider the written data is). more insights into LLMs is insights into us. maybe read motivation for some interpretability articles
% G: but this is not so much within the scope of this study
Given that LLMs are trained of a large corpus of human data, what they know says a lot about humanity (at least what is captured in text). Improving our understanding of the inner representations of LLMs yields insights about us.

% G: also these tools can be applied to other DNNs that handle audio image video

\section{Related Work}

% This section briefly summarizes the main results from other papers which inspired my analysis and also explain some of the results obtained. It also should reinforce the idea that it's justifiable to treat LLMs and chaotic nonlinear systems. 

The application of dynamical systems theory to neural networks has revealed complex behaviors in modern architectures.  

\ref{li2025cognitive_activation} proposes that LLMs' reasoning capabilities as chaotic processes of dynamic information extraction in parameter space, introducing Quasi-Lyapunov Exponents to quantify chaotic characteristics across model layers, and showing sensitivity to initial conditions.

\paragraph{Attention Mechanisms and Nonlinear Dynamics.}
\ref{geshkovski2025mathematicalperspectivetransformers} model self-attention as a nonlinear coupling between tokens.
\ref{poc-lopez2024dynamical_mean_field_self_attention} demonstrate that even simplified self-attention transformer networks with 1-bit tokens and weights exhibit nontrivial dynamical phenomena, including nonequilibrium phase transitions and chaotic bifurcations. 

\paragraph{Determinism and Attractors in Language Models.}
% The tension between reproducibility and novely in LLMs has become increasingly relevant. 
\ref{he2025nondeterminism} (GGG)
Recent engineering efforts have focused on achieving deterministic inference by controlling GPU batching variance and floating-point errors, motivated by the need for reproducibility and clear signals in reinforcement learning training. 
 %Knowing the LLMs sensitivity to disturbances is important for this.
 %This highlights the practical sensitivity of LLMs to small numerical perturbations. 

\ref{wang2025unveiling_attractor_cycles} show that LLMs used for paraphrasing converge to periodic attractor cycles, reducing linguistic diversity.

\ref{cyclegan} % Comment: PLOS Complex Systems article - 
demonstrate that CycleGAN image generators' space is more limited than the training data's with Positive Lyapunov exponents and attractor dimensions similar to the training data intrinsic dimension. Chaotic dynamics contribute to the diversity of the generated images.
%This suggests that generative models may naturally converge to stable configurations that reduce output diversity. 
% G: this is a bit contradictory bc I say that model generation: more limited than training data but chaotic makes more diverse.

\paragraph{Interpretability Through Dynamical Analysis.}
%The dynamical systems perspective has proven valuable for understanding recurrent architectures. 
\ref{sussillo2013} showed that fixed points and linearized dynamics in trained RNNs provide interpretable insights into network function. \ref{zhang2024intelligence_edge_of_chaos} found that intelligence emerges at an optimal complexity level. Systems that are either highly chaotic or perfectly periodic exhibit poor downstream performance, suggesting a "sweet spot" conducive to intelligent behavior at the edge between order and chaos.


% normalization layers
\ref{tomihari2025recurrent_self_attention_dynamics} show that normalization layers normallize Jacobian's complex eigenvalues bringing the dynamics close to a critical state with maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos: a critical regime where signals neither explode nor vanish, enabling long-range information propagation.
% G: this can explain if the trajectories have sub-exponential divergence
%show that normalization layers effectively suppress the Jacobian's spectral norm and control oscillatory behaviors. Their empirical findings reveal that high-performance self-attention models exhibit maximum Lyapunov exponents close to zero, suggesting operation at the edge of chaos—a critical regime where signals neither explode nor vanish, enabling long-range information propagation.






\section{Preliminaries}

% This section describes the background knowledge required to understand Nonlinear Dynamics and Chaos, LLMs, and their similarities and differences. This provides us with the starting point for this paper.

\subsection{Nonlinear Dynamics and Chaos}
A deterministic dynamical system is one whose future evolution is uniquely determined by its current state. Such systems are often modeled by a set of coupled ordinary differential equations (ODEs) of the form:
\begin{equation}
    \frac{d\mathbf{x}}{dt} = f(\mathbf{x}(t))
\end{equation}
where $\mathbf{x}(t) \in \mathbb{R}^n$ is the state vector in an $n$-dimensional phase space, and $f$ is a nonlinear function. The sequence of states over time, $\mathbf{x}(t)$, traces a path known as a trajectory in this phase space.

Chaotic systems are a fascinating subclass of deterministic nonlinear systems that exhibit complex and unpredictable behavior. This apparent randomness arises not from stochastic inputs, but from the system's intrinsic dynamics. The defining characteristic of chaos is \textbf{sensitivity to initial conditions (SIC)}. This property implies that trajectories originating from infinitesimally close initial states, say $\mathbf{x}(0)$ and $\mathbf{x}(0) + \bm{\delta}(0)$, will diverge exponentially over time. The separation distance $\|\bm{\delta}(t)\|$ grows according to:
\begin{equation}
    \|\bm{\delta}(t)\| \approx \|\bm{\delta}(0)\| e^{\lambda t}
\end{equation}
where $\lambda$ is the maximal Lyapunov exponent. A positive maximal Lyapunov exponent is a strong indicator of chaos. It signifies that any measurement error or uncertainty in the initial state, no matter how small, will be amplified exponentially, rendering long-term prediction impossible.

Despite their unpredictability, the trajectories of dissipative chaotic systems are not entirely random; they are confined to a bounded, lower dimensional subregion in phase space called a \textbf{strange attractor}. These attractors often possess a complex, self-similar geometry known as a fractal structure. The dimensionality of this structure can be characterized by a non-integer value. A common measure is the \textbf{correlation dimension ($D_2$)}. It is based on the correlation integral, $C(\epsilon)$, which measures the probability that two points on the attractor are closer than a distance $\epsilon$. For small $\epsilon$, the correlation integral scales as a power law:
\begin{equation}
    C(\epsilon) = \lim_{N \to \infty} \frac{2}{N(N-1)} \sum_{i<j} \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|) \sim \epsilon^{D_2}
\end{equation}
where $N$ is the number of points on the trajectory and $\Theta$ is the Heaviside step function. The dimension $D_2$ is then found as the slope of the linear region in a log-log plot of $C(\epsilon)$ versus $\epsilon$.

Another powerful tool for analyzing dynamical systems is \textbf{Recurrence Analysis}. A Recurrence Plot (RP) visualizes the times at which a trajectory revisits a previously visited neighborhood in phase space. The recurrence matrix is defined as:
\begin{equation}
    R_{i,j}(\epsilon) = \Theta(\epsilon - \|\mathbf{x}_i - \mathbf{x}_j\|), \quad i,j = 1, \dots, N
\end{equation}
% G: any distance metric can be used
The visual patterns in an RP provide deep insight: periodic systems produce long, parallel diagonal lines, random systems produce a uniform, noisy plot, and chaotic systems produce a complex geometric structure with short, interrupted diagonal lines. These structures can be quantified using Recurrence Quantification Analysis (RQA).


% G: I would like to generally describe the necessary conditions for chaos. (I dont mean SIC and exponential divergence, but rather the underlying characteristics of the system which will make it be chaotic). Then I'd like to explain which components of LLM architecture correspond to these. example: nonlinear coupling and attention

In general, the following structural conditions of a system cause it to be chaotic:
Nonlinearity: % GGG
Stretching and Folding mechanism: Stretching separates close trajectories, causing SIC, while folding confines them to a bounded subset of the state space, a strange attractor.
\cite{Hnon1976ATM}
\cite{ROSSLER1976}
\cite{strogatz_textbook}


\subsection{Large Language Models (LLMs)}
Large Language Models are a class of deep neural networks, with the Transformer architecture being the de facto standard. They are trained on massive text corpora to predict the probability distribution for the next token in a sequence. They are autoregressive models used to generate a text output sequentially.

The process begins with tokenization, where input text is broken down into a sequence of integers (tokens). Each token is then mapped to a high-dimensional vector, its embedding. These embeddings are processed through a stack of Transformer layers. Each layer is primarily composed of two sub-modules: a multi-head self-attention mechanism and a position-wise feed-forward network (MLP). The self-attention mechanism is the key source of nonlinearity and contextual understanding, allowing the model to dynamically weigh the importance of all other tokens in the context window when updating the representation of a single token. It can be expressed as:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
% TODO: explain what each variable is. concise. definition + intuition. also show how it updates the hidden states
This operation creates a powerful, nonlinear feedback and coupling mechanism between all elements of the sequence. The MLP further processes these representations through additional nonlinear transformations. Normalization is (GGG). 
% The output of the final layer is a sequence of hidden states, which represent the contextualized meaning of each token. % G: clarify that for all layers we can get it and also connect it better to how the logits are obtained

During inference, the LLM generates text autoregressively, which can be viewed as a discrete-time dynamical system:
\begin{equation}
    \mathbf{s}_{t+1} = f(\mathbf{s}_t, \mathbf{s}_{t-1}, ..., \mathbf{s}_0)
\end{equation}
where $\mathbf{s}_t$ represents the model's internal state (the sequence of hidden states) at step $t$. To generate the next token, the hidden state of the last token is passed through a final linear layer (the "un-embedding") to produce logits over the entire vocabulary, which are then converted to a probability distribution via a softmax function. A decoding strategy is then used to select a token. \textbf{Greedy sampling} deterministically selects the token with the highest probability. In contrast, \textbf{probabilistic sampling} introduces stochasticity, often controlled by a temperature parameter ($T$). Higher temperatures flatten the distribution, making the output more diverse but less coherent.

\subsection{Layer normalization}
% G: mention other normalizations? like batch?
Layer normalization (LayerNorm) acts on a single hidden state vector \(h\in\mathbb{R}^d\) (e.g. a token embedding) by (i) mean-centering, (ii) variance normalization, and (iii) a learned affine anisotropic rescaling.
\begin{align}
\mu(h) &\;=\; \frac{1}{d}\sum_{i=1}^d h_i, \\
\sigma^2(h) &\;=\; \frac{1}{d}\sum_{i=1}^d (h_i-\mu(h))^2, \\
\widehat{h} &\;=\; \frac{h - \mu(h)\mathbf{1}}{\sqrt{\sigma^2(h)+\varepsilon}}, \label{eq:ln_norm}\\
y &\;=\; \gamma \odot \widehat{h} + \beta \;=\; D\widehat{h} + \beta, \label{eq:ln_affine}
\end{align}
where \(\mathbf{1}=(1,\dots,1)^\top , \gamma, \beta \in\mathbb{R}^d\), and \(D=\operatorname{diag}(\gamma)\).
% G: pretty unreadable. 
% benefits:
% Introducing a normalization of the hidden states after every operation as part of the model architecture made th training between 4 and 20 times faster \cite{loshchilov2025ngptnormalizedtransformerrepresentation}
% Representation learning on the hypersphere leads to more stable training, greater embedding space separability, and better performance on downstream tasks \cite{wang2022understandingcontrastiverepresentationlearning}
% Normalization techniques are beneficial \cite{salimans2016weightnormalizationsimplereparameterization}

% G: maybe we can hypothesize whether norm keeps the model more stable by limiting lyapunov exponents and keeping it at the edge of chaos

\begin{itemize}
  \item \(h\in\mathbb{R}^d\): input hidden state (one token, one sample).
  \item \(\mu(h)\): coordinate mean of \(h\)
  \item \(\sigma^2(h)\): empirical coordinate variance of \(h\)
  %\item \(\widehat{h}\): normalized (zero-mean, unit-variance) vector
  \item \(\gamma\in\mathbb{R}^d\) (or \(D=\mathrm{diag}(\gamma)\in\mathbb{R}^{d\times d}\)): per-coordinate learned scale (anisotropic linear part)
  \item \(\beta\in\mathbb{R}^d\): learned bias
  \item \(\varepsilon\): numerical stabilizer
\end{itemize}

\paragraph{Geometric remarks}
\begin{enumerate}
  \item The centering step \(h\mapsto h-\mu(h)\mathbf{1}\) projects onto the hyperplane orthogonal to \(\mathbf1\) (removes the mean direction).
  \item The variance normalization (\ref{eq:ln_norm}) is nonlinear and couples all coordinates: it maps each centered vector to a point on a sphere:
  \[
    \lVert \widehat{h}\rVert_2 = \sqrt{d}.
  \]
  Thus, before the affine step the image lies on the \((d-2)\)-sphere inside the mean-zero hyperplane (degree-of-freedom count: \(d\to d-2\)).
  \item The affine step \(y=D\widehat{h}+\beta\) turns the sphere into an ellipsoid (and then translates it); \(D\) rescales coordinate axes independently and does not introduce coordinate mixing.
\end{enumerate}

\paragraph{Jacobian (local linearization)}
Let \(u(h):=h-\mu(h)\mathbf1\) and \(\|u\|=\sqrt{u^\top u}\).  A convenient form of the Jacobian of the normalization map \(\widehat{h}(h)\) is
\[
J_{\widehat{h}}(h)
\;=\;
\frac{\sqrt{d}}{\|u\|}\,\Big(I - \frac{u u^\top}{\|u\|^2}\Big)\Big(I - \frac{1}{d}\mathbf1\mathbf1^\top\Big),
\]
and the full LayerNorm Jacobian (including \(D\)) is \(J_y(h)=D\,J_{\widehat{h}}(h)\).  For generic \(h\) (with \(d>2\)) this linearization has rank \(d-2\): two null directions correspond to the mean direction \(\mathbf1\) and the radial direction \(u\) (the latter is collapsed by the radial normalization).
% G: By killing the 𝑢 u-direction, LayerNorm throws away “how much” the features deviate from the mean and keeps only “in what relative proportions” they deviate.
% G: could emphasize that the radial direction u is specific for every h, but the mean centering is global.

\paragraph{Interpretation for dynamics}
LayerNorm is therefore a structured \emph{many-to-one} nonlinear map: it (i) removes one linear degree of freedom (the mean), (ii) collapses the radial coordinate in the centered hyperplane (folding onto a compact manifold), and (iii) applies an anisotropic linear rescaling and shift.
% G: (ii) is too much
% In dynamical terms this provides a strong folding/bounding operation; any stretching/expansion must come from the other components (linear layers, attention, pointwise nonlinearities) for the overall layer-to-layer map to exhibit expansion+folding behavior.


\subsection{Analogy and Limitations}
We propose viewing the autoregressive process of an LLM as a high-dimensional, discrete-time dynamical system. The sequence of hidden states serves as the trajectory evolving in the model's internal phase space. The self-attention mechanism acts as a (state-dependent) nonlinear coupling between different tokens. 
% Each token's representation is affected by (previous) other tokens, with softmax, ReLU, layer norm providing the nonlinearity.

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Chaotic System} & \textbf{Large Language Model (LLM)} \\ \midrule
Phase Space & Embedding / Hidden State Space \\
State Vector & Hidden State Vector \\
Phase Space Trajectory & Sequence os Token Text or Hidden States \\
Continuous Time Evolution & Discrete Token Generation Step \\
Differential Equation & Autoregressive Function \\
Numerical Solution & Forward Pass Inference \\
Nonlinear Coupling & Self-Attention Mechanism \\
Initial Condition & Initial Prompt Embedding \\
Measurement Uncertainty & Floating Point Precision, Quantization \\ \bottomrule
\end{tabular}
\caption{Analogies between concepts in chaos theory and LLMs.}
\label{tab:analogy}
\end{table}
% G: could add stretching and folding. table too big rn

However, the analogy is not perfect and has critical limitations. LLM dynamics are discrete, not continuous, with a fixed "time step" of one token. The act of sampling a token from the probability distribution and appending it to the input can be seen as a "collapse" or a sharp discontinuity in the trajectory's evolution. 
% G: can clarify that it matters in that this new token is the starting point for the processing in the 1st layer, but all the previous hidden states remain and will influence at each layer.
LLMs are usually used with probabilistic sampling, which adds a random component and makes it non-deterministic. Furthermore, the dimensionality of the state space is extraordinarily high (thousands of dimensions), which poses significant challenges for traditional chaos analysis techniques.

% G: also in chaotic systems the nonlinear coupling is between variables, but in llm it is between tokens, which would be the state of the system (so all the variables) at different times

% G: as i understand it. MLP provides nonlinearity, attention nonlinear coupling and (maybe) stretching. normalization keeps the system bounded (folding). layer norm is more like projecting to the plane perpendicular to (1,1,1,..,1) plus affine rescale.

% G: is normalization folding or stretching? normalize would siggest constraining so folding, but attention is cross communication so also folding. talk this out and dicuss. which processes in LLM constrain and maintain boudedness? is this folding? which stretch= is stretching individual?

\section{Methods}

\subsection{Initial Condition Generation}
To test for sensitivity to initial conditions, we generate a set of trajectories from infinitesimally close starting points. We begin with a single prompt, tokenize it, and retrieve its initial embedding tensor $\mathbf{E}_0 \in \mathbb{R}^{L \times D}$, where $L$ is the prompt length and $D$ is the model's hidden dimension. We then create a "blob" (GGG) of $N$ perturbed initial conditions by adding a small, random perturbation tensor $\in \mathbb{R}^{L \times D}$ of fixed magnitude (radius $r$). Then we use LLM to continue (GGG generate) each trajectory.
% G: idk if perturbation is the best word here. disturbance?

\subsection{Trajectory Distance Metrics}
We use a suite of metrics to quantify the divergence between pairs of trajectories, $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n)$ and $\mathbf{Y} = (\mathbf{y}_1, ..., \mathbf{y}_m)$. Distance between vector representations of text encode meaning about its semantic similarity.
% G: encode? similarty/closeness? improve last sentence
% G: cite word2vec and some paper with clustering from LLM or show my clustering. also some from text2vec embedders
% G: also mention that some metrics are based on other metrics: like rank eigen uses cos similarity to compare pairs of vectors. check which use L2
% We justify the use of cosine similarity as it is a meaningful metric both in the hidden state and embedded text space.

\subsubsection{Vector-wise Metrics}
These metrics are applied pointwise between corresponding vectors of two trajectories, yielding a time series of distance values.
\begin{itemize}
    \item \textbf{Cosine Distance:} Measures the angle between two vectors, capturing their orientation similarity. It is defined as $1 - \text{Cosine Similarity}$:
    \begin{equation}
        d_{cos}(\mathbf{a}, \mathbf{b}) = 1 - \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}
    \end{equation}
    \item \textbf{Normalized Cross-Correlation:} Measures the similarity between two vectors as a function of the displacement of one relative to the other. For discrete vectors at zero lag, it is related to the cosine similarity.
\end{itemize}

\subsubsection{Trajectory-wise Metrics}
These metrics yield a single scalar value for the distance between two entire trajectories. To obtain a divergence curve, we apply these metrics in a sliding window of increasing size.
\begin{itemize}
    \item \textbf{Dynamic Time Warping (DTW):} Finds an optimal non-linear alignment between two sequences. It computes a cost matrix where $C_{i,j}$ is the distance between $\mathbf{x}_i$ and $\mathbf{y}_j$. The DTW distance is the value in the final cell of an accumulated cost matrix $D$, defined by the recurrence:
    \begin{equation}
        D_{i,j} = C_{i,j} + \min(D_{i-1, j}, D_{i, j-1}, D_{i-1, j-1})
    \end{equation}
    \item \textbf{Fréchet Distance:} Intuitively, the minimum length of a leash needed to connect a person and a dog walking along their respective paths. It is sensitive to the ordering of points. For two curves $P$ and $Q$, it is:
    \begin{equation}
        d_F(P, Q) = \inf_{\alpha, \beta} \max_{t \in [0,1]} \| P(\alpha(t)) - Q(\beta(t)) \|
    \end{equation}
    where $\alpha$ and $\beta$ are non-decreasing reparameterizations.
    \item \textbf{Hausdorff Distance:} Measures the greatest of all distances from a point in one set to the closest point in the other set. It is a pure set-based distance.
    \begin{equation}
        d_H(X, Y) = \max \left( \sup_{\mathbf{x} \in X} \inf_{\mathbf{y} \in Y} \|\mathbf{x}-\mathbf{y}\|, \sup_{\mathbf{y} \in Y} \inf_{\mathbf{x} \in X} \|\mathbf{x}-\mathbf{y}\| \right)
    \end{equation}
\end{itemize}

\subsubsection{Structural Metric}
\begin{itemize}
        
    \item \textbf{SVD Eigenvector Ranking Deviation:} To compare the structural properties of two trajectories, we first compute the covariance matrix for each sequence of hidden states. We then perform Singular Value Decomposition (SVD) on the centered data matrices, yielding principal directions (eigenvectors $V^{(i)}$) for each trajectory.

    To compare the two sets of eigenvectors, we compute the cosine similarity matrix.
    \[
    S_{ab} = \frac{v^{(1)}_a \cdot v^{(2)}_b}{\|v^{(1)}_a\| \|v^{(2)}_b\|}
    \]
    where $v^{(1)}_a$ and $v^{(2)}_b$ are the $a$-th and $b$-th eigenvectors of the first and second trajectory, respectively.

    For each eigenvector $v^{(1)}_a$, we find the index $b^*$ of the most similar eigenvector in the second trajectory:
    \[
    b^* = \arg\max_b S_{ab}
    \]

    We can show the ranking of eigenvectors to show how similar two trajectories are. The closer to the proportionality line, the more similar two trajectories.
    % TODO: Plot diagonal

    We then compute the sum of cosine distances for these matched pairs:
    \[
    D_{\text{sum\_cos\_dist}} = \sum_{a=1}^k \left[1 - S_{a b^*}\right]
    \]
    where $D_{\text{sum\_cos\_dist}}$ is the scalar metric quantifying the dissimilarity between the two trajectories.

    \textbf{Symmetry:} This metric is \emph{not symmetric}. Matching is performed from the eigenvectors of the first trajectory to those of the second, and the assignment is not necessarily reciprocal. That is, $D_{\text{sum\_cos\_dist}}(X^{(1)}, X^{(2)}) \neq D_{\text{sum\_cos\_dist}}(X^{(2)}, X^{(1)})$ in general.




\end{itemize}

\subsection{Recurrence Analysis}
We generate Recurrence Plots (RPs) using the self-similarity matrix of a single hidden state trajectory with all the distance metrics. We analyze the plots qualitatively and quantitatively using Recurrence Quantification Analysis (RQA) metrics, including:
\begin{itemize}
    \item \textbf{Determinism (DET):} The percentage of recurrence points forming diagonal lines, indicating predictability.
    \item \textbf{Laminarity (LAM):} The percentage of recurrence points forming vertical lines, indicating periods of stable or "laminar" states.
    \item \textbf{Entropy (ENT):} The Shannon entropy of the distribution of diagonal line lengths, reflecting the complexity of the deterministic structure.
\end{itemize}
% G: could show example how cos sim matrix is calculated and the shape for clarity

\subsection{Dimensionality}
We estimate the correlation dimension ($D_2$) of the trajectories using the Grassberger-Procaccia algorithm. This can be calculated from the self-similarity matrix by applying different thresholds (which correspond to the distance $\epsilon$). 
% G: expand. mention that when T != 0, the fits are bad, but T = 0 (at last layer), fit is good. correlation dimension can be used to identify  deterministic (chaotic) systems.

\subsection{Clustering}

Furthermore, we apply clustering algorithms to the set of hidden state vectors within a trajectory to identify distinct operational states or regions in the model's high-dimensional phase space.

% G: either do an Experiments section or here I need to conretely explain what I did: model choice, params, dim, fix length generation, context window, etc...
% brief comment about T

\section(Results)

\subsection{Trajectory Divergence and Lyapunov} % GGG

There were some challenges inherent to the system under study. In any bounded (GGG) system, there is a maximum possible value for the distance, and the distance between two diverging trajectories will eventually reach the maximum and saturate. In our case, especially when using sentence embeddings, the number of datapoints until the saturation region was often small (GGG around 6), making any type of curve fitting difficult.
% G: am i using the word bounded properly

\subsection{RP}

Strong similarities can be seen between the RP from the Lorez system \ref{XXX} and from the LLMs output.


Chaotic features are more strongly present at the last layers of the LLM as showed in the RP and pointwise dimension. This is due to the progressive addition of more nonlinearity and coupling at each layer. 
% As pointwise dimension is calculated from RP, it makes sense that they agree on this.

% not always at the last layer, but also middle layers are quite chaotic

\section{Conclusions and Discussion}

% \subsection{Possible Explanations for LLMs being chaotic}

% (rough) invariance for different metrics {hausdorf, frechet, dtw} (and also embedders). discuss. they literally look the same. I mean to be fair the data is the same. quite trivial

% mention the challenges and successes with high dim data. also distance metrics. and the ones I invented

\section{Acknowledgements }

I would like to thank my supervisors Dr. János Török and Kristóf Benedek for their help and support during this work.

\newpage
\bibliographystyle{unsrt}
\bibliography{lit}

\section{Appendix }

\subsection{Intuition behind different metrics}

In order to gain a more intuitive understanding of each metric (GGG and maybe embedding too), we show RP from the same trajectory.


% G: Here i want to show how embeddings kind of do some averaging and pooling and context, and how some metrics pick up different features, some more global some more detail. 
% G: mention window size

\subsection{Handling outliers}


\end{document}

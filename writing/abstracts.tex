---- DRAFT 2

Large Language Models (LLMs) have achieved remarkable performance across a wide range of tasks, yet their internal dynamics remain poorly understood. In this work, we apply the tools of nonlinear dynamics and chaos theory to LLMs. By analyzing both text and hidden state trajectories, we demonstrate that LLMs exhibit hallmark signatures of chaos, including sensitivity to initial conditions and a positive maximum Lyapunov exponent, with consistent results across different distance metrics. Recurrence plots show structural similarities between LLMs and canonical chaotic systems such as the Lorenz attractor, while dimension analysis reveals fractal structures in the hidden state space, particularly pronounced in the last layers. We propose that the nonlinear coupling induced by attention mechanisms plays a key role in driving this chaotic behavior. 


---- DRAFT 1


% Large Language Models (LLMs) have become an increasingly important part of our lives. 
Large Language Models (LLMs) are powerful models that have achived impressive performance on a wide range of tasks.
% K: Large Language Models (LLMs) are powerful tools that have achieved impressive performance on a wide range of tasks, such as natural language processing, computer vision, graphics, and many more.
Despite these developments, there is limited theoretical understanding of the internal dynamics of the models. In this paper, we establish a close connection between LLMs and chaotic systems both in the text and hidden state space using the tools from Nonlinear Dynamics and Chaos Theory. We show that LLMs 
%text and hidden state 
trajectories exhibit sensitivity to initial conditions (SIC) and a positive maximum lyapunov exponent. They share structural similarities with other typical chaotic systems such as Lorenz systems as evidenced by the Recurrence Plots (RP). Pointwise and Correlation Dimension analysis shows fractal structures in the hidden state space. These chaotic features are especially expressed in the LLM's last layers. We discuss how challenges associated with the high dimensionality of the data can be handled and which distance metrics are suitable for such an analysis.

% G: i know the dimension stuff is not well defined yet
% G: Also id like to mention something that we propose. Ex: We propose that attention provides the nonlinear coupling necessary for systems to be chaotic and diverge exponentially... etc
% G: should i clarify the scope of the study: llm transformers with deepseek architecture?

% K: I collected your thoughts and rephrased a bit. it is exactly what you wrote, but maybe a bit nicer wording/more coherent. Feel free to use it as inspiration.

% Large Language Models (LLMs) have achieved remarkable performance across a wide range of tasks, yet their internal dynamics remain poorly understood. In this work, we approach LLMs through the lens of nonlinear dynamics and chaos theory, establishing a close connection between these models and chaotic systems. By analyzing both text and hidden state trajectories, we demonstrate that LLMs exhibit hallmark signatures of chaos, including sensitivity to initial conditions and a positive maximum Lyapunov exponent. Recurrence plots reveal structural similarities between LLM dynamics and canonical chaotic systems such as the Lorenz attractor, while dimension analysis uncovers fractal-like structures within the hidden state spaceâ€”particularly pronounced in the final layers. We propose that the nonlinear coupling induced by attention mechanisms plays a key role in driving this chaotic behavior. Beyond providing a new theoretical perspective, our findings suggest that chaos-theoretic tools can offer valuable insights for interpreting, evaluating, and potentially guiding the future design of LLM architectures.




---- RANDOM SENTENCES

Large Language Models (LLMs) have become an increasingly important part of our lives.

Despite these developments, little is known about the internal dynamics of the model.
% ///
Despite these developments, there is limiteed theoretical understanding of the internal dynamics of the model.
% and the current studies are limited to 
% G: here i am referring mainly to interpretability and circuits in LLMs studies




In this paper we show that LLMs text and hidden state trajectories exhibit sensitivity to initial conditions (SIC) and a positive maximum lyapunov exponent. They also share structural similarities with other typical chaotic systems such as Lorenz systems as evidenced by the Recurrence Plots (RP), with the last layers of the model being more chaotic. We discuss how challenges associated with the high dimensionality of the data can be handled and which distance metrics are suitable for such an analysis.

These results show that LLMs posses many features of chaotic systems, which is relevant for future development in the field.
% ///
We establish a close connection between LLMs and chaotic systems both in the text and hidden state space.


Code is available at: 


% pointwise dimension ?

These are highly nonlinear models
Chaos theory studies deterministic nonlinear models

We believe the tools from Nonlinear Dynamics and Chaos Theory provide valuable insights into the inner representations of LLMs.

We believe studying LLMs as nonlinear systems using the tools from Chaos Theory is interesting





Using Recurrence Plots (and RQA) we found that the LLM's hidden state trajectories share similarities with typical chaotic systems like Lorenz systems, and that they exhibit sensitivity to initial conditions (SIC) and positive maximum lyapunov exponent.











